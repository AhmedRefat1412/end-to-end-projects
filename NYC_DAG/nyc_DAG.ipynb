{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55415040",
   "metadata": {},
   "source": [
    "اول حاجه هنحط الداتا في مونجو دي بي "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9eaa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 41362 documents into MongoDB successfully\n"
     ]
    }
   ],
   "source": [
    "#this is new DAG\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "\n",
    "#  الاتصال بـ MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "#  اختيار Database و Collection\n",
    "db = client[\"nyc_data\"]\n",
    "collection = db[\"nyc_traffic\"]\n",
    "\n",
    "# (اختياري) تفريغ الكوليكشن لو بتجرب\n",
    "# collection.delete_many({})\n",
    "\n",
    "#  قراءة ملف الـ JSON\n",
    "with open(\"nyc_traffic.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# تأكيد إن الداتا Array\n",
    "assert isinstance(data, list), \"JSON file must contain a list of documents\"\n",
    "\n",
    "#  إدخال الدhتا\n",
    "collection.insert_many(data)\n",
    "\n",
    "print(f\" Inserted {len(data)} documents into MongoDB successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf13acc",
   "metadata": {},
   "source": [
    "حولنا الداتا بتتاعت الشوارع من ملف سي اي في اي  ملف  parquet\n",
    "ودي عشان ننوع من مصارد الداتا الي جايلنا "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9960acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "data = pd.read_csv(\"Centerline.csv\")\n",
    "data.to_parquet(\"streets_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99794ffa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b10bd2e",
   "metadata": {},
   "source": [
    "انشاء اول تاسك في الداج وده هنعمل كونكت مع الاس 3 و هنكريت باكت \n",
    "ونعمل فيها 2 فولدر وبعدين نرفع الرو داتا فيها \n",
    "1- هنجيب الداتا  من مونجو دي بي ونرفعها علي هئيه جيسون فايل \n",
    "2- هنجيب الفايل البركيه ونرفعه \n",
    "3- هنجيب الفايل الثالث والاخير ونرفعه الي هو csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e009768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "# اتصال Mongo\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client[\"nyc_data\"]\n",
    "collection = db[\"nyc_traffic\"]\n",
    "\n",
    "#-----------------------------------------------\n",
    "# connect with s3 \n",
    "s3 = boto3.resource(\n",
    "    's3',\n",
    "    endpoint_url='http://localhost:9010',\n",
    "    aws_access_key_id='minioadmin',\n",
    "    aws_secret_access_key='minioadmin123'\n",
    ")\n",
    "Bucket=\"datalake\"\n",
    "\n",
    "#------------------------------------------------\n",
    "# create bucket لو مش موجود \n",
    "try:\n",
    "     s3.create_bucket(Bucket=\"datalake\")\n",
    "except:\n",
    "     pass\n",
    "\n",
    "#------------------------------------------------\n",
    "# create 2 folder (raw , processed)\n",
    "try:\n",
    "     s3.put_object(Bucket=\"datalake\", Key=\"raw/\")\n",
    "     s3.put_object(Bucket=\"datalake\", Key=\"processed/\")\n",
    "except:\n",
    "     pass\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# upload first file to s3 (json file)\n",
    "cursor = collection.find({}, {\"_id\": 0})\n",
    "\n",
    "data = list(cursor)          # in-memory\n",
    "json_bytes = json.dumps(\n",
    "    data,\n",
    "    ensure_ascii=False\n",
    ").encode(\"utf-8\")\n",
    "\n",
    "#load to s3\n",
    "s3.Bucket(\"datalake\").put_object(\n",
    "    Key=\"raw/mongo_traffic.json\",\n",
    "    Body=json_bytes,\n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "\n",
    "\n",
    "#---------------------------------------------------==\n",
    "# upload second file csv to s3\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='http://localhost:9010',\n",
    "    aws_access_key_id='minioadmin',\n",
    "    aws_secret_access_key='minioadmin123'\n",
    ")\n",
    "\n",
    "\n",
    "s3_client.upload_file(\n",
    "    Filename=\"Motor_Vehicle_Collisions_Crashes.csv\",  # المسار المحلي للفايل\n",
    "    Bucket=\"datalake\",                    # اسم الباكيت في S3/MinIO\n",
    "    Key=\"raw/Motor_Vehicle_Collisions_Crashes.csv\",    # المسار داخل الباكيت\n",
    "    ExtraArgs={\"ContentType\": \"text/csv\"}   # دي زي ميتا داتا بعرفه ايه الكونتنت الي جوا الفايل ده \n",
    "\n",
    ")\n",
    "\n",
    "#--------------------------------------\n",
    "# upload the last file (parquet file )\n",
    "\n",
    "s3_client.upload_file(\n",
    "    Filename=\"streets_data.parquet\",  # المسار المحلي للفايل\n",
    "    Bucket=\"datalake\",                    # اسم الباكيت في S3/MinIO\n",
    "    Key=\"raw/streets_data.parquet\" ,   # المسار داخل الباكيت\n",
    "    ExtraArgs={\"ContentType\": \"application/octet-stream\"}  # دي زي ميتا داتا بعرفه ايه الكونتنت الي جوا الفايل ده \n",
    "\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d7dc27",
   "metadata": {},
   "source": [
    "التاسك التاني هنجيب الداتا بتاعت الجيسون فايل ونحولها لداتا فريم \n",
    "دا بستخدام باي سبارك و نعمل عليها بروسيسنج وبعدين نرفعها علي س3 تاني بس المره ده في فولدر اسمه بروسيس داتا \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6499759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/10 18:17:32 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "26/01/10 18:17:32 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "26/01/10 18:17:32 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "[Stage 58:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+--------------------+-------------------+-----------------+---------+----------+---------------------+--------+------+---------------+--------------------+-----------------+-----------------+\n",
      "|  borough|created_date|fatalities|         incident_id|      incident_type|injuries_reported| latitude| longitude|response_time_minutes|severity|status|    street_name|traffic_volume_level|vehicles_involved|weather_condition|\n",
      "+---------+------------+----------+--------------------+-------------------+-----------------+---------+----------+---------------------+--------+------+---------------+--------------------+-----------------+-----------------+\n",
      "|   queens|  2025-01-01|         0|99a27f0e-637a-45d...| signal malfunction|                0|40.758144|-73.800187|                   17|  medium|  open|     2nd avenue|               Light|                3|            clear|\n",
      "| brooklyn|  2025-01-01|         0|8424a97b-e059-4e6...|    illegal parking|                0|  40.6638|-73.900441|                   26|     low|  open| madison avenue|               Light|                3|            clear|\n",
      "|   queens|  2025-01-01|         0|1019cad0-6f98-4f4...| traffic congestion|                2|40.773744|-73.847663|                   15|  medium|  open|  fulton street|               Light|                4|            clear|\n",
      "|   queens|  2025-01-01|         0|cdf2451e-e8a8-45f...| traffic congestion|                0| 40.77803|-73.808294|                   20|  medium|closed|flatbush avenue|            Moderate|                3|            clear|\n",
      "|manhattan|  2025-01-01|         0|ea22f9b7-639f-430...| traffic congestion|                0|40.729824| -73.97233|                   20|  medium|  open|     5th avenue|               Light|                2|            clear|\n",
      "|   queens|  2025-01-01|         1|0d852896-92f1-4d5...|   vehicle accident|                4|40.751354|-73.828865|                    9|    high|closed|flatbush avenue|            Moderate|                5|            clear|\n",
      "|manhattan|  2025-01-01|         0|408f0fc1-b077-414...|    illegal parking|                1|40.728593|-73.978977|                   45|     low|closed|     4th avenue|               Light|                2|            clear|\n",
      "|manhattan|  2025-01-01|         0|5ab4ddd0-9d32-474...|   vehicle accident|                0|40.740995|  -73.9703|                   14|  medium|closed|  fulton street|               Light|                4|            clear|\n",
      "|manhattan|  2025-01-01|         0|f19ac05f-701a-43b...| traffic congestion|                0|40.733018|-73.958154|                   15|  medium|  open|     5th avenue|            Moderate|                1|            clear|\n",
      "|   queens|  2025-01-01|         1|c75b3a91-27b4-4f8...|pedestrian accident|                3|40.756056|-73.843845|                    8|critical|closed|    park avenue|               Heavy|                4|            clear|\n",
      "+---------+------------+----------+--------------------+-------------------+-----------------+---------+----------+---------------------+--------+------+---------------+--------------------+-----------------+-----------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# task 2\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col , lower ,to_date\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"transform1\")\n",
    "\n",
    "    # ✅ Hadoop S3A jars \n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.4.0,\"\n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
    "    )\n",
    "\n",
    "    # MinIO\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9010\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "#spark.sparkContext.setLogLevel(\"WARN\")\n",
    "# هنجيب الداتا بقا ونحولها لداتا فريم\n",
    "\n",
    "df = spark.read.json(\n",
    "    \"s3a://datalake/raw/mongo_traffic.json\"\n",
    ")\n",
    "\n",
    "\n",
    "# تعديل القيم في الاعمده لتصبح كلها سمول \n",
    "\n",
    "cols_to_lower = [\"borough\", \"incident_type\", \"severity\", \"status\", \"street_name\", \"weather_condition\"]\n",
    "\n",
    "for c in cols_to_lower:\n",
    "    df = df.withColumn(c, lower(col(c)))\n",
    "\n",
    "# تغير الداتا تايب بتاع الديت \n",
    "\n",
    "df1 = df.withColumn(\n",
    "    \"created_date\", to_date(col(\"created_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "\n",
    "df1.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .csv(\"s3a://datalake/processed/mongo_traffic\")\n",
    "\n",
    "df1.show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60e9be3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- borough: string (nullable = true)\n",
      " |-- created_date: date (nullable = true)\n",
      " |-- fatalities: long (nullable = true)\n",
      " |-- incident_id: string (nullable = true)\n",
      " |-- incident_type: string (nullable = true)\n",
      " |-- injuries_reported: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- response_time_minutes: long (nullable = true)\n",
      " |-- severity: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- street_name: string (nullable = true)\n",
      " |-- traffic_volume_level: string (nullable = true)\n",
      " |-- vehicles_involved: long (nullable = true)\n",
      " |-- weather_condition: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b12ce97",
   "metadata": {},
   "source": [
    "التاسك التاني \n",
    "ودي عباره عن اننا هنجيب الفايل الي هو csv \n",
    "وهنبدء نتعامل معاه علي pyspark \n",
    " بحيث اننا نعمل بروسيسنج لداتا الي فيه بما يتناسب مع احتياجتنا \n",
    " وفي الاخر نرفع الفايل علي هئيه بركيه فايل علي s3\n",
    "جوا الفولدر الي اسمه processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8ccaa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+---------+------------+-----------------+---------------+--------------+--------------------+\n",
      "|collision_id|crash_date|crash_time|  borough|vehicle_type|      street_name|persons_injured|persons_killed| contributing_factor|\n",
      "+------------+----------+----------+---------+------------+-----------------+---------------+--------------+--------------------+\n",
      "|     4803256|2025-04-02|     09:01|   queens|   ambulance|   unknown street|              0|             0|         unspecified|\n",
      "|     4804525|2025-04-07|     12:01|    bronx|       sedan|bedford park blvd|              0|             0|         unspecified|\n",
      "|     4804466|2025-04-07|     06:01|    bronx|       sedan|   unknown street|              0|             0|        unsafe speed|\n",
      "|     4790397|2025-01-31|     08:01|   queens|       sedan|        exeter st|              1|             0|driver inattentio...|\n",
      "|     4790246|2025-02-02|     20:01|  unknown|       sedan|   unknown street|              3|             0|aggressive drivin...|\n",
      "|     4790510|2025-01-28|     17:01|manhattan|        taxi|   unknown street|              0|             0|         unspecified|\n",
      "|     4789833|2025-02-02|     16:01|   queens| chassis cab|   unknown street|              0|             0|    backing unsafely|\n",
      "|     4805018|2025-04-07|     12:01|   queens|       sedan|           164 st|              0|             0|         unspecified|\n",
      "|     4805711|2025-04-11|     20:01|    bronx|       sedan|   unknown street|              0|             0|driver inattentio...|\n",
      "|     4805744|2025-04-11|     16:01|  unknown|        subn|triborough bridge|              0|             0|unsafe lane changing|\n",
      "+------------+----------+----------+---------+------------+-----------------+---------------+--------------+--------------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/10 17:08:29 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "26/01/10 17:08:29 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "26/01/10 17:08:29 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "26/01/10 17:08:39 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "26/01/10 17:08:39 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "26/01/10 17:08:48 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# task 3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import to_date, col ,date_format , to_timestamp , lower\n",
    "\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"transform2\")\n",
    "\n",
    "    # ✅ Hadoop S3A jars \n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.4.0,\"\n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
    "    )\n",
    "\n",
    "    # MinIO\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9010\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n",
    "#==============================================================\n",
    "# هنجيب الداتا  ونحولها لداتا فريم\n",
    "\n",
    "df = spark.read.csv(\n",
    "    \"s3a://datalake/raw/Motor_Vehicle_Collisions_Crashes.csv\",\n",
    "    inferSchema=True,\n",
    "    header=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# معالجه الداتا الي في الفايل \n",
    "#----------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# تحدبد الاعمده الي محتجنيها بس \n",
    "df_selected=df.select(\"COLLISION_ID\",\"CRASH DATE\",\"CRASH TIME\",\"BOROUGH\",\"VEHICLE TYPE CODE 1\"\n",
    "              ,\"ON STREET NAME\",\"NUMBER OF PERSONS INJURED\",\"NUMBER OF PERSONS KILLED\"\n",
    "              , \"CONTRIBUTING FACTOR VEHICLE 1\") \n",
    "\n",
    "#----------------------------------\n",
    "# تغير الداتا تيب بتاع عمود التاريخ والقيم الي مش هتفع تبقا تاريخ تيبقا نل\n",
    "df1 = df_selected.withColumn(\n",
    "    \"CRASH DATE\",\n",
    "    expr(\"try_to_date(`CRASH DATE`, 'M/d/yyyy')\")\n",
    ")\n",
    "\n",
    "#----------------------------------\n",
    "#  في عمود الديت هنعمل فلتره بحيث نجيب  القيم الي مش بنل \n",
    "df2 = df1.filter(col(\"CRASH DATE\").isNotNull())\n",
    "\n",
    "#----------------------------------\n",
    "# فلتره الداتا لعام 2025 بس \n",
    "df_2025_plus = df2.filter(\n",
    "    col(\"CRASH DATE\") > \"2025-01-01\"\n",
    ")\n",
    "\n",
    "\n",
    "#----------------------------------\n",
    "# هنحاول اننا نعدل القيم الفاضيه بنل دي بعدين ان كان في وقت \n",
    "# بس دلوقتي هنحط  القيم الفاضيه دي unknown\n",
    "df_fill = df_2025_plus.fillna({\n",
    "    \"BOROUGH\": \"UNKNOWN\",          # أي Null في BOROUGH هيتحول لـ \"UNKNOWN\"\n",
    "    \"ON STREET NAME\": \"UNKNOWN STREET\" , # أي Null في ON STREET NAME هيتحول لـ \"UNKNOWN STREET\"\n",
    "    \"NUMBER OF PERSONS INJURED\":\"0\", # عدد الاصابات الي بنل هحط مكانه 0\n",
    "    \"VEHICLE TYPE CODE 1\":\"Sedan\" # عندي 1417 قيمه  بنل هعوضهم  ب اكتر قيمه اتكرت \n",
    "    \n",
    "})\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# تحويل الداتا تايب بتاع العمود الاصابات الي  int \n",
    "df_4 = df_fill.withColumn(\n",
    "    \"NUMBER OF PERSONS INJURED\",\n",
    "    col(\"NUMBER OF PERSONS INJURED\").cast(\"int\")\n",
    ")\n",
    "from pyspark.sql.functions import to_timestamp , date_format\n",
    "#----------------------------------\n",
    "# تحويل الداتا الي في العمود بتاع الوقت لصيغه ديت\n",
    "df_time = df_4.withColumn(\n",
    "    \"CRASH TIME\",\n",
    "    to_timestamp(col(\"CRASH TIME\"), \"H:mm\")\n",
    ")\n",
    "\n",
    "#----------------------------------\n",
    "# HH:MM تحديد ان الديت يبقا بصيغه الساعه والدقائق فقط  \n",
    "df_final = df_time.withColumn(\n",
    "    \"CRASH TIME\",\n",
    "    date_format(\"CRASH TIME\", \"HH:MM\")\n",
    ")\n",
    "\n",
    "#=========================================\n",
    "# تغير اسماء الاعمده\n",
    "rename_dict = {\n",
    "    \"CRASH DATE\": \"crash_date\",\n",
    "    \"CRASH TIME\": \"crash_time\",\n",
    "    \"ON STREET NAME\": \"street_name\",\n",
    "    \"NUMBER OF PERSONS INJURED\": \"persons_injured\",\n",
    "    \"NUMBER OF PERSONS KILLED\": \"persons_killed\",\n",
    "    \"VEHICLE TYPE CODE 1\": \"vehicle_type\",\n",
    "    \"CONTRIBUTING FACTOR VEHICLE 1\": \"contributing_factor\",\n",
    "    \"COLLISION_ID\": \"collision_id\",\n",
    "    \"BOROUGH\": \"borough\"\n",
    "}\n",
    "\n",
    "for old, new in rename_dict.items():\n",
    "    df_final = df_final.withColumnRenamed(old, new)\n",
    "\n",
    "# هنحول الداتا الي في الاعمده كلها لسمول  \n",
    "df_final = df_final.withColumn(\"borough\", lower(col(\"borough\")))\n",
    "df_final = df_final.withColumn(\"vehicle_type\", lower(col(\"vehicle_type\")))\n",
    "df_final = df_final.withColumn(\"street_name\", lower(col(\"street_name\")))\n",
    "df_final = df_final.withColumn(\"contributing_factor\", lower(col(\"contributing_factor\")))\n",
    "\n",
    "df_final.show(10)\n",
    "\n",
    "\n",
    "\n",
    "# هنحفظ الفايل الجديد علي س3 في الفولدر بتاع الداتا المتعالجه \n",
    "df_final.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .csv(\"s3a://datalake/processed/Motor_Vehicle_Collisions_Crashes\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72029545",
   "metadata": {},
   "source": [
    "task4\n",
    "------\n",
    "هنجيب الداتا للفايل الثالث من s3\n",
    "وهنعمل بروسيسنج علي الفايل ده برضو \n",
    "وبعدين نرفع الفايل تاني علي s3\n",
    "في فولدر الداتا بروسيسد"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48b9d956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-----------------+-------------------+-------------+------------+---------+\n",
      "|       street_name|           global_id|number_park_lanes|number_travel_lanes|snow_priority|street_width|bike_lane|\n",
      "+------------------+--------------------+-----------------+-------------------+-------------+------------+---------+\n",
      "|                 n|cedc2dde-7e8b-442...|                2|                  2|            C|          34|        0|\n",
      "|              hone|9c163e85-23ad-418...|                2|                  2|            S|          32|        0|\n",
      "|                48|fdccf94f-201f-431...|                2|                  1|            S|          32|        0|\n",
      "|            laight|bcbbb800-b963-45b...|                2|                  1|            S|          34|        0|\n",
      "|             brook|d7f8c5d8-637b-412...|                1|                  2|            H|          25|        0|\n",
      "|                60|27fcb089-c93b-41f...|                2|                  2|            C|          50|        0|\n",
      "|fordham university|fd57d250-48c4-4cb...|                0|                  1|      UNKNOWN|          12|        0|\n",
      "|               210|7505f297-28f8-4de...|                2|                  2|            S|          30|        0|\n",
      "|    booth memorial|d0e23276-a779-4ff...|                1|                  2|            C|          40|        0|\n",
      "|           nameoke|2504359f-bc51-4ea...|                2|                  2|            S|          24|        0|\n",
      "+------------------+--------------------+-----------------+-------------------+-------------+------------+---------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/10 17:16:12 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "26/01/10 17:16:12 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "26/01/10 17:16:12 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# task 4\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import to_date, col ,date_format , to_timestamp , lower\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"transform3\")\n",
    "\n",
    "    # ✅ Hadoop S3A jars \n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.4.0,\"\n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
    "    )\n",
    "\n",
    "    # MinIO\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9010\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n",
    "#==============================================================\n",
    "# هنجيب الداتا  ونحولها لداتا فريم\n",
    "\n",
    "df = spark.read.csv(\n",
    "    \"s3a://datalake/raw/Centerline.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "#--------------------------------------\n",
    "#هنجيب الاعمده الي محتجينها بس \n",
    "df=df.select(\"STREET NAME\",\"GlobalID\",\"Number Park Lanes\",\"Number Travel Lanes\",\"Snow Priority\",\n",
    "          \"Street Width\",\"BIKE_LANE\")\n",
    "\n",
    "#--------------------------------------\n",
    "# fillna\n",
    "df=df.fillna(\n",
    "    {\n",
    "        \"Number Park Lanes\":0, # تعويض الارقام الي ب نل ب 0\n",
    "        \"Number Travel Lanes\":2,\n",
    "        \"Snow Priority\":\"UNKNOWN\",\n",
    "        \"Street Width\":30,\n",
    "        \"BIKE_LANE\":0\n",
    "\n",
    "    }\n",
    ")\n",
    "#--------------------------------------\n",
    "# تغير اسماء الاعمده \n",
    "rename_dic={\n",
    "    \"STREET NAME\":\"street_name\",\n",
    "    \"GlobalID\":\"global_id\",\n",
    "    \"Number Park Lanes\":\"number_park_lanes\",\n",
    "    \"Number Travel Lanes\":\"number_travel_lanes\",\n",
    "    \"Snow Priority\":\"snow_priority\",\n",
    "    \"Street Width\":\"street_width\",\n",
    "    \"BIKE_LANE\":\"bike_lane\"\n",
    "\n",
    "}\n",
    "\n",
    "for old , new in rename_dic.items():\n",
    "     df=df.withColumnRenamed(old, new)\n",
    "#  هنحول القيم الي في الاعمده الاسترينج الي قيم كلها سمول \n",
    "df = df.withColumn(\"street_name\",lower(col(\"street_name\")))\n",
    "\n",
    "\n",
    "df.show(10)\n",
    "#--------------------------------------\n",
    "# هنحفظ الفايل الجديد علي س3 في الفولدر بتاع الداتا المتعالجه \n",
    "df.write \\\n",
    "     .mode(\"overwrite\") \\\n",
    "     .option(\"header\",\"true\") \\\n",
    "     .csv(\"s3a://datalake/processed/streets_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f116988",
   "metadata": {},
   "source": [
    "#Task 5\n",
    " هنجيب الداتا كلها ونبدء نعمل جوينز عشان نكون الداتا وير هاوس \n",
    " هنرفع الفاكت والديمنشنز علي s3  \n",
    " في فولدر جديد اسمه dw \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a759959d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/10 17:02:07 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- street_name: string (nullable = true)\n",
      " |-- global_id: string (nullable = true)\n",
      " |-- number_park_lanes: integer (nullable = true)\n",
      " |-- number_travel_lanes: integer (nullable = true)\n",
      " |-- snow_priority: string (nullable = true)\n",
      " |-- street_width: integer (nullable = true)\n",
      " |-- bike_lane: integer (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- collision_id: integer (nullable = true)\n",
      " |-- crash_date: date (nullable = true)\n",
      " |-- crash_time: timestamp (nullable = true)\n",
      " |-- borough: string (nullable = true)\n",
      " |-- vehicle_type: string (nullable = true)\n",
      " |-- street_name: string (nullable = true)\n",
      " |-- persons_injured: integer (nullable = true)\n",
      " |-- persons_killed: integer (nullable = true)\n",
      " |-- contributing_factor: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#task 5\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"dw\")\n",
    "\n",
    "    # ✅ Hadoop S3A jars \n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.4.0,\"\n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
    "    )\n",
    "\n",
    "    # MinIO\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9010\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# هنجيب اول داتا ونحطها في داتا فريم streets_data\n",
    "streets_data = spark.read.csv(\n",
    "    \"s3a://datalake/processed/streets_data\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# هنجيب ثاني  داتا ونحطها في داتا فريم Motor_Vehicle_Collisions_Crashes\n",
    "Motor_Vehicle = spark.read.csv(\n",
    "    \"s3a://datalake/processed/Motor_Vehicle_Collisions_Crashes\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# هنجيب ثالث  داتا ونحطها في داتا فريم mongo_traffic\n",
    "mongo_traffic = spark.read.csv(\n",
    "    \"s3a://datalake/processed/mongo_traffic\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "\n",
    "print(streets_data.printSchema())\n",
    "print(Motor_Vehicle.printSchema())\n",
    "# create dim_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c204ea4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- borough: string (nullable = true)\n",
      " |-- created_date: date (nullable = true)\n",
      " |-- fatalities: integer (nullable = true)\n",
      " |-- incident_id: string (nullable = true)\n",
      " |-- incident_type: string (nullable = true)\n",
      " |-- injuries_reported: integer (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- response_time_minutes: integer (nullable = true)\n",
      " |-- severity: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- street_name: string (nullable = true)\n",
      " |-- traffic_volume_level: string (nullable = true)\n",
      " |-- vehicles_involved: integer (nullable = true)\n",
      " |-- weather_condition: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(mongo_traffic.printSchema())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a1304f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|      borough|count|\n",
      "+-------------+-----+\n",
      "|        Bronx| 8377|\n",
      "|     Brooklyn| 8292|\n",
      "|       Queens| 8273|\n",
      "|    Manhattan| 8245|\n",
      "|Staten Island| 8175|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|      borough|count|\n",
      "+-------------+-----+\n",
      "|     brooklyn|23126|\n",
      "|       queens|17122|\n",
      "|      unknown|16268|\n",
      "|    manhattan|12664|\n",
      "|        bronx| 9747|\n",
      "|staten island| 2647|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "mongo_traffic.groupBy(\"borough\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"count\", ascending=False) \\\n",
    "  .show(20,truncate=True)\n",
    "\n",
    "\n",
    "Motor_Vehicle.groupBy(\"borough\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"count\", ascending=False) \\\n",
    "  .show(20,truncate=True)\n",
    "#df_fill.filter(col(\"CRASH TIME\").isNull()).count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark)",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
