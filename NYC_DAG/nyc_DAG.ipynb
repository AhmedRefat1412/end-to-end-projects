{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55415040",
   "metadata": {},
   "source": [
    "اول حاجه هنحط الداتا في مونجو دي بي "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9eaa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 41362 documents into MongoDB successfully\n"
     ]
    }
   ],
   "source": [
    "#this is new DAG\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "\n",
    "#  الاتصال بـ MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "#  اختيار Database و Collection\n",
    "db = client[\"nyc_data\"]\n",
    "collection = db[\"nyc_traffic\"]\n",
    "\n",
    "# (اختياري) تفريغ الكوليكشن لو بتجرب\n",
    "# collection.delete_many({})\n",
    "\n",
    "#  قراءة ملف الـ JSON\n",
    "with open(\"nyc_traffic.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# تأكيد إن الداتا Array\n",
    "assert isinstance(data, list), \"JSON file must contain a list of documents\"\n",
    "\n",
    "#  إدخال الدhتا\n",
    "collection.insert_many(data)\n",
    "\n",
    "print(f\" Inserted {len(data)} documents into MongoDB successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf13acc",
   "metadata": {},
   "source": [
    "حولنا الداتا بتتاعت الشوارع من ملف سي اي في اي  ملف  parquet\n",
    "ودي عشان ننوع من مصارد الداتا الي جايلنا "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9960acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "data = pd.read_csv(\"Centerline.csv\")\n",
    "data.to_parquet(\"streets_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99794ffa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b10bd2e",
   "metadata": {},
   "source": [
    "انشاء اول تاسك في الداج وده هنعمل كونكت مع الاس 3 و هنكريت باكت \n",
    "ونعمل فيها 2 فولدر وبعدين نرفع الرو داتا فيها \n",
    "1- هنجيب الداتا  من مونجو دي بي ونرفعها علي هئيه جيسون فايل \n",
    "2- هنجيب الفايل البركيه ونرفعه \n",
    "3- هنجيب الفايل الثالث والاخير ونرفعه الي هو csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e009768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "# اتصال Mongo\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client[\"nyc_data\"]\n",
    "collection = db[\"nyc_traffic\"]\n",
    "\n",
    "#-----------------------------------------------\n",
    "# connect with s3 \n",
    "s3 = boto3.resource(\n",
    "    's3',\n",
    "    endpoint_url='http://localhost:9010',\n",
    "    aws_access_key_id='minioadmin',\n",
    "    aws_secret_access_key='minioadmin123'\n",
    ")\n",
    "Bucket=\"datalake\"\n",
    "\n",
    "#------------------------------------------------\n",
    "# create bucket لو مش موجود \n",
    "try:\n",
    "     s3.create_bucket(Bucket=\"datalake\")\n",
    "except:\n",
    "     pass\n",
    "\n",
    "#------------------------------------------------\n",
    "# create 2 folder (raw , processed)\n",
    "try:\n",
    "     s3.put_object(Bucket=\"datalake\", Key=\"raw/\")\n",
    "     s3.put_object(Bucket=\"datalake\", Key=\"processed/\")\n",
    "except:\n",
    "     pass\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# upload first file to s3 (json file)\n",
    "cursor = collection.find({}, {\"_id\": 0})\n",
    "\n",
    "data = list(cursor)          # in-memory\n",
    "json_bytes = json.dumps(\n",
    "    data,\n",
    "    ensure_ascii=False\n",
    ").encode(\"utf-8\")\n",
    "\n",
    "#load to s3\n",
    "s3.Bucket(\"datalake\").put_object(\n",
    "    Key=\"raw/mongo_traffic.json\",\n",
    "    Body=json_bytes,\n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "\n",
    "\n",
    "#---------------------------------------------------==\n",
    "# upload second file csv to s3\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='http://localhost:9010',\n",
    "    aws_access_key_id='minioadmin',\n",
    "    aws_secret_access_key='minioadmin123'\n",
    ")\n",
    "\n",
    "\n",
    "s3_client.upload_file(\n",
    "    Filename=\"Motor_Vehicle_Collisions_Crashes.csv\",  # المسار المحلي للفايل\n",
    "    Bucket=\"datalake\",                    # اسم الباكيت في S3/MinIO\n",
    "    Key=\"raw/Motor_Vehicle_Collisions_Crashes.csv\",    # المسار داخل الباكيت\n",
    "    ExtraArgs={\"ContentType\": \"text/csv\"}   # دي زي ميتا داتا بعرفه ايه الكونتنت الي جوا الفايل ده \n",
    "\n",
    ")\n",
    "\n",
    "#--------------------------------------\n",
    "# upload the last file (parquet file )\n",
    "\n",
    "s3_client.upload_file(\n",
    "    Filename=\"streets_data.parquet\",  # المسار المحلي للفايل\n",
    "    Bucket=\"datalake\",                    # اسم الباكيت في S3/MinIO\n",
    "    Key=\"raw/streets_data.parquet\" ,   # المسار داخل الباكيت\n",
    "    ExtraArgs={\"ContentType\": \"application/octet-stream\"}  # دي زي ميتا داتا بعرفه ايه الكونتنت الي جوا الفايل ده \n",
    "\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d7dc27",
   "metadata": {},
   "source": [
    "التاسك التاني هنجيب الداتا بتاعت الجيسون فايل ونحولها لداتا فريم \n",
    "دا بستخدام باي سبارك و نعمل عليها بروسيسنج وبعدين نرفعها علي س3 تاني بس المره ده في فولدر اسمه بروسيس داتا \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6499759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/04 19:36:11 WARN Utils: Your hostname, refat, resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "26/01/04 19:36:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/ahmed-refat/miniconda3/envs/spark/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/ahmed-refat/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/ahmed-refat/.ivy2.5.2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-704627fd-5328-4dc3-b9c2-009a0d02a9a5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.4.0 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.23.19 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.1.3.Final in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      ":: resolution report :: resolve 846ms :: artifacts dl 21ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.4.0 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.1.3.Final from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.23.19 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-704627fd-5328-4dc3-b9c2-009a0d02a9a5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/17ms)\n",
      "26/01/04 19:36:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# task 2\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"transform1\")\n",
    "\n",
    "    # ✅ Hadoop S3A jars \n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.4.0,\"\n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
    "    )\n",
    "\n",
    "    # MinIO\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9010\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "#spark.sparkContext.setLogLevel(\"WARN\")\n",
    "# هنجيب الداتا بقا ونحولها لداتا فريم\n",
    "\n",
    "df = spark.read.json(\n",
    "    \"s3a://datalake/raw/mongo_traffic.json\"\n",
    ")\n",
    "\n",
    "#df.show(20)\n",
    "#----------------------------------------------------------\n",
    "# هنرجع الداتا تاني في الفولدر الي هو processed\n",
    "#  وهنخلي الفايل يبقا parquet  عشان يحتفظ ب الاسيكما احسن من csv \n",
    "\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"s3a://datalake/processed/mongo_traffic\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b12ce97",
   "metadata": {},
   "source": [
    "التاسك التاني \n",
    "ودي عباره عن اننا هنجيب الفايل الي هو csv \n",
    "وهنبدء نتعامل معاه علي pyspark \n",
    " بحيث اننا نعمل بروسيسنج لداتا الي فيه بما يتناسب مع احتياجتنا \n",
    " وفي الاخر نرفع الفايل علي هئيه بركيه فايل علي s3\n",
    "جوا الفولدر الي اسمه processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ccaa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+---------+------------+-----------------+---------------+--------------+--------------------+\n",
      "|collision_id|crash_date|crash_time|  borough|vehicle_type|      street_name|persons_injured|persons_killed| contributing_factor|\n",
      "+------------+----------+----------+---------+------------+-----------------+---------------+--------------+--------------------+\n",
      "|     4803256|2025-04-02|     09:01|   QUEENS|   Ambulance|   UNKNOWN STREET|              0|             0|         Unspecified|\n",
      "|     4804525|2025-04-07|     12:01|    BRONX|       Sedan|BEDFORD PARK BLVD|              0|             0|         Unspecified|\n",
      "|     4804466|2025-04-07|     06:01|    BRONX|       Sedan|   UNKNOWN STREET|              0|             0|        Unsafe Speed|\n",
      "|     4790397|2025-01-31|     08:01|   QUEENS|       Sedan|        EXETER ST|              1|             0|Driver Inattentio...|\n",
      "|     4790246|2025-02-02|     20:01|  UNKNOWN|       Sedan|   UNKNOWN STREET|              3|             0|Aggressive Drivin...|\n",
      "|     4790510|2025-01-28|     17:01|MANHATTAN|        Taxi|   UNKNOWN STREET|              0|             0|         Unspecified|\n",
      "|     4789833|2025-02-02|     16:01|   QUEENS| Chassis Cab|   UNKNOWN STREET|              0|             0|    Backing Unsafely|\n",
      "|     4805018|2025-04-07|     12:01|   QUEENS|       Sedan|           164 ST|              0|             0|         Unspecified|\n",
      "|     4805711|2025-04-11|     20:01|    BRONX|       Sedan|   UNKNOWN STREET|              0|             0|Driver Inattentio...|\n",
      "|     4805744|2025-04-11|     16:01|  UNKNOWN|        subn|TRIBOROUGH BRIDGE|              0|             0|Unsafe Lane Changing|\n",
      "+------------+----------+----------+---------+------------+-----------------+---------------+--------------+--------------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# task 3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import to_date, col ,date_format , to_timestamp\n",
    "\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"transform3\")\n",
    "\n",
    "    # ✅ Hadoop S3A jars \n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.4.0,\"\n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
    "    )\n",
    "\n",
    "    # MinIO\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9010\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n",
    "#==============================================================\n",
    "# هنجيب الداتا  ونحولها لداتا فريم\n",
    "\n",
    "df = spark.read.csv(\n",
    "    \"s3a://datalake/raw/Motor_Vehicle_Collisions_Crashes.csv\",\n",
    "    inferSchema=True,\n",
    "    header=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# معالجه الداتا الي في الفايل \n",
    "#----------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# تحدبد الاعمده الي محتجنيها بس \n",
    "df_selected=df.select(\"COLLISION_ID\",\"CRASH DATE\",\"CRASH TIME\",\"BOROUGH\",\"VEHICLE TYPE CODE 1\"\n",
    "              ,\"ON STREET NAME\",\"NUMBER OF PERSONS INJURED\",\"NUMBER OF PERSONS KILLED\"\n",
    "              , \"CONTRIBUTING FACTOR VEHICLE 1\") \n",
    "\n",
    "#----------------------------------\n",
    "# تغير الداتا تيب بتاع عمود التاريخ والقيم الي مش هتفع تبقا تاريخ تيبقا نل\n",
    "df1 = df_selected.withColumn(\n",
    "    \"CRASH DATE\",\n",
    "    expr(\"try_to_date(`CRASH DATE`, 'M/d/yyyy')\")\n",
    ")\n",
    "\n",
    "#----------------------------------\n",
    "#  في عمود الديت هنعمل فلتره بحيث نجيب  القيم الي مش بنل \n",
    "df2 = df1.filter(col(\"CRASH DATE\").isNotNull())\n",
    "\n",
    "#----------------------------------\n",
    "# فلتره الداتا لعام 2025 بس \n",
    "df_2025_plus = df2.filter(\n",
    "    col(\"CRASH DATE\") > \"2025-01-01\"\n",
    ")\n",
    "\n",
    "\n",
    "#----------------------------------\n",
    "# هنحاول اننا نعدل القيم الفاضيه بنل دي بعدين ان كان في وقت \n",
    "# بس دلوقتي هنحط  القيم الفاضيه دي unknown\n",
    "df_fill = df_2025_plus.fillna({\n",
    "    \"BOROUGH\": \"UNKNOWN\",          # أي Null في BOROUGH هيتحول لـ \"UNKNOWN\"\n",
    "    \"ON STREET NAME\": \"UNKNOWN STREET\" , # أي Null في ON STREET NAME هيتحول لـ \"UNKNOWN STREET\"\n",
    "    \"NUMBER OF PERSONS INJURED\":\"0\", # عدد الاصابات الي بنل هحط مكانه 0\n",
    "    \"VEHICLE TYPE CODE 1\":\"Sedan\" # عندي 1417 قيمه  بنل هعوضهم  ب اكتر قيمه اتكرت \n",
    "    \n",
    "})\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# تحويل الداتا تايب بتاع العمود الاصابات الي  int \n",
    "df_4 = df_fill.withColumn(\n",
    "    \"NUMBER OF PERSONS INJURED\",\n",
    "    col(\"NUMBER OF PERSONS INJURED\").cast(\"int\")\n",
    ")\n",
    "from pyspark.sql.functions import to_timestamp , date_format\n",
    "#----------------------------------\n",
    "# تحويل الداتا الي في العمود بتاع الوقت لصيغه ديت\n",
    "df_time = df_4.withColumn(\n",
    "    \"CRASH TIME\",\n",
    "    to_timestamp(col(\"CRASH TIME\"), \"H:mm\")\n",
    ")\n",
    "\n",
    "#----------------------------------\n",
    "# HH:MM تحديد ان الديت يبقا بصيغه الساعه والدقائق فقط  \n",
    "df_final = df_time.withColumn(\n",
    "    \"CRASH TIME\",\n",
    "    date_format(\"CRASH TIME\", \"HH:MM\")\n",
    ")\n",
    "\n",
    "#=========================================\n",
    "# تغير اسماء الاعمده\n",
    "rename_dict = {\n",
    "    \"CRASH DATE\": \"crash_date\",\n",
    "    \"CRASH TIME\": \"crash_time\",\n",
    "    \"ON STREET NAME\": \"street_name\",\n",
    "    \"NUMBER OF PERSONS INJURED\": \"persons_injured\",\n",
    "    \"NUMBER OF PERSONS KILLED\": \"persons_killed\",\n",
    "    \"VEHICLE TYPE CODE 1\": \"vehicle_type\",\n",
    "    \"CONTRIBUTING FACTOR VEHICLE 1\": \"contributing_factor\",\n",
    "    \"COLLISION_ID\": \"collision_id\",\n",
    "    \"BOROUGH\": \"borough\"\n",
    "}\n",
    "\n",
    "for old, new in rename_dict.items():\n",
    "    df_final = df_final.withColumnRenamed(old, new)\n",
    "\n",
    "df_final.show(10)\n",
    "\n",
    "\n",
    "# هنحفظ الفايل الجديد علي س3 في الفولدر بتاع الداتا المتعالجه \n",
    "df_final.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"s3a://datalake/processed/Motor_Vehicle_Collisions_Crashes\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72029545",
   "metadata": {},
   "source": [
    "task4\n",
    "------\n",
    "هنجيب الداتا للفايل الثالث من s3\n",
    "وهنعمل بروسيسنج علي الفايل ده برضو \n",
    "وبعدين نرفع الفايل تاني علي s3\n",
    "في فولدر الداتا بروسيسد"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48b9d956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/05 23:13:56 ERROR Executor: Exception in task 0.0 in stage 19.0 (TID 36)\n",
      "org.apache.spark.SparkException: [FAILED_READ_FILE.NO_HINT] Encountered error while reading file s3a://datalake/processed/Motor_Vehicle_Collisions_Crashes/part-00004-58ee1b9a-b947-4138-99ed-363815062d90-c000.snappy.parquet.  SQLSTATE: KD001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:911)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:141)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:773)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.RuntimeException: java.lang.NoSuchMethodError: 'java.util.List org.apache.hadoop.fs.VectoredReadUtils.validateNonOverlappingAndReturnSortedRanges(java.util.List)'\n",
      "\tat org.apache.parquet.util.DynMethods$UnboundMethod.invokeChecked(DynMethods.java:67)\n",
      "\tat org.apache.parquet.hadoop.util.wrapped.io.VectorIoBridge.readWrappedRanges(VectorIoBridge.java:244)\n",
      "\tat org.apache.parquet.hadoop.util.wrapped.io.VectorIoBridge.readVectoredRanges(VectorIoBridge.java:201)\n",
      "\tat org.apache.parquet.hadoop.util.H1SeekableInputStream.readVectored(H1SeekableInputStream.java:75)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readVectored(ParquetFileReader.java:1357)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readAllPartsVectoredOrNormal(ParquetFileReader.java:1274)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.internalReadRowGroup(ParquetFileReader.java:1185)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:1135)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readNextFilteredRowGroup(ParquetFileReader.java:1380)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase$ParquetRowGroupReaderImpl.readNextRowGroup(SpecificParquetRecordReaderBase.java:292)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:481)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:399)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:238)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:130)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:292)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:130)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:139)\n",
      "\t... 22 more\n",
      "Caused by: java.lang.NoSuchMethodError: 'java.util.List org.apache.hadoop.fs.VectoredReadUtils.validateNonOverlappingAndReturnSortedRanges(java.util.List)'\n",
      "\tat org.apache.hadoop.fs.s3a.S3AInputStream.readVectored(S3AInputStream.java:895)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.readVectored(FSDataInputStream.java:308)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat org.apache.parquet.util.DynMethods$UnboundMethod.invokeChecked(DynMethods.java:62)\n",
      "\t... 39 more\n",
      "26/01/05 23:13:56 WARN TaskSetManager: Lost task 0.0 in stage 19.0 (TID 36) (10.0.2.15 executor driver): org.apache.spark.SparkException: [FAILED_READ_FILE.NO_HINT] Encountered error while reading file s3a://datalake/processed/Motor_Vehicle_Collisions_Crashes/part-00004-58ee1b9a-b947-4138-99ed-363815062d90-c000.snappy.parquet.  SQLSTATE: KD001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:911)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:141)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:773)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.RuntimeException: java.lang.NoSuchMethodError: 'java.util.List org.apache.hadoop.fs.VectoredReadUtils.validateNonOverlappingAndReturnSortedRanges(java.util.List)'\n",
      "\tat org.apache.parquet.util.DynMethods$UnboundMethod.invokeChecked(DynMethods.java:67)\n",
      "\tat org.apache.parquet.hadoop.util.wrapped.io.VectorIoBridge.readWrappedRanges(VectorIoBridge.java:244)\n",
      "\tat org.apache.parquet.hadoop.util.wrapped.io.VectorIoBridge.readVectoredRanges(VectorIoBridge.java:201)\n",
      "\tat org.apache.parquet.hadoop.util.H1SeekableInputStream.readVectored(H1SeekableInputStream.java:75)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readVectored(ParquetFileReader.java:1357)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readAllPartsVectoredOrNormal(ParquetFileReader.java:1274)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.internalReadRowGroup(ParquetFileReader.java:1185)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:1135)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readNextFilteredRowGroup(ParquetFileReader.java:1380)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase$ParquetRowGroupReaderImpl.readNextRowGroup(SpecificParquetRecordReaderBase.java:292)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:481)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:399)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:238)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:130)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:292)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:130)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:139)\n",
      "\t... 22 more\n",
      "Caused by: java.lang.NoSuchMethodError: 'java.util.List org.apache.hadoop.fs.VectoredReadUtils.validateNonOverlappingAndReturnSortedRanges(java.util.List)'\n",
      "\tat org.apache.hadoop.fs.s3a.S3AInputStream.readVectored(S3AInputStream.java:895)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.readVectored(FSDataInputStream.java:308)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat org.apache.parquet.util.DynMethods$UnboundMethod.invokeChecked(DynMethods.java:62)\n",
      "\t... 39 more\n",
      "\n",
      "26/01/05 23:13:56 ERROR TaskSetManager: Task 0 in stage 19.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o221.showString.\n: org.apache.spark.SparkException: [FAILED_READ_FILE.NO_HINT] Encountered error while reading file s3a://datalake/processed/Motor_Vehicle_Collisions_Crashes/part-00004-58ee1b9a-b947-4138-99ed-363815062d90-c000.snappy.parquet.  SQLSTATE: KD001\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:911)\n\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:142)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:141)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:773)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1017)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2496)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2517)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2536)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2275)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1401)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2265)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2263)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:176)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:284)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:138)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:138)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:307)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:137)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:91)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:249)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2263)\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1401)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2814)\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:338)\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:374)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.RuntimeException: java.lang.NoSuchMethodError: 'java.util.List org.apache.hadoop.fs.VectoredReadUtils.validateNonOverlappingAndReturnSortedRanges(java.util.List)'\n\tat org.apache.parquet.util.DynMethods$UnboundMethod.invokeChecked(DynMethods.java:67)\n\tat org.apache.parquet.hadoop.util.wrapped.io.VectorIoBridge.readWrappedRanges(VectorIoBridge.java:244)\n\tat org.apache.parquet.hadoop.util.wrapped.io.VectorIoBridge.readVectoredRanges(VectorIoBridge.java:201)\n\tat org.apache.parquet.hadoop.util.H1SeekableInputStream.readVectored(H1SeekableInputStream.java:75)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readVectored(ParquetFileReader.java:1357)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readAllPartsVectoredOrNormal(ParquetFileReader.java:1274)\n\tat org.apache.parquet.hadoop.ParquetFileReader.internalReadRowGroup(ParquetFileReader.java:1185)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:1135)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readNextFilteredRowGroup(ParquetFileReader.java:1380)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase$ParquetRowGroupReaderImpl.readNextRowGroup(SpecificParquetRecordReaderBase.java:292)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:481)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:399)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:238)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:130)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:292)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:130)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:139)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:773)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: java.lang.NoSuchMethodError: 'java.util.List org.apache.hadoop.fs.VectoredReadUtils.validateNonOverlappingAndReturnSortedRanges(java.util.List)'\n\tat org.apache.hadoop.fs.s3a.S3AInputStream.readVectored(S3AInputStream.java:895)\n\tat org.apache.hadoop.fs.FSDataInputStream.readVectored(FSDataInputStream.java:308)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat org.apache.parquet.util.DynMethods$UnboundMethod.invokeChecked(DynMethods.java:62)\n\t... 39 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 36\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#==============================================================\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# هنجيب الداتا  ونحولها لداتا فريم\u001b[39;00m\n\u001b[1;32m     31\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3a://datalake/processed/Motor_Vehicle_Collisions_Crashes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m     inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     34\u001b[0m )\n\u001b[0;32m---> 36\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# هنحفظ الفايل الجديد علي س3 في الفولدر بتاع الداتا المتعالجه \u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# df_final.write \\\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#     .mode(\"overwrite\") \\\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#     .parquet(\"s3a://datalake/processed/streets_data\")\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark/lib/python3.10/site-packages/pyspark/sql/classic/dataframe.py:285\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark/lib/python3.10/site-packages/pyspark/sql/classic/dataframe.py:303\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    298\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    299\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    300\u001b[0m     )\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/spark/lib/python3.10/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/spark/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:263\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    265\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark/lib/python3.10/site-packages/py4j/protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o221.showString.\n: org.apache.spark.SparkException: [FAILED_READ_FILE.NO_HINT] Encountered error while reading file s3a://datalake/processed/Motor_Vehicle_Collisions_Crashes/part-00004-58ee1b9a-b947-4138-99ed-363815062d90-c000.snappy.parquet.  SQLSTATE: KD001\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:911)\n\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:142)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:141)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:773)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1017)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2496)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2517)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2536)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2275)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1401)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2265)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2263)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:176)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:284)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:138)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:138)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:307)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:137)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:91)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:249)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2263)\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1401)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2814)\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:338)\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:374)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.RuntimeException: java.lang.NoSuchMethodError: 'java.util.List org.apache.hadoop.fs.VectoredReadUtils.validateNonOverlappingAndReturnSortedRanges(java.util.List)'\n\tat org.apache.parquet.util.DynMethods$UnboundMethod.invokeChecked(DynMethods.java:67)\n\tat org.apache.parquet.hadoop.util.wrapped.io.VectorIoBridge.readWrappedRanges(VectorIoBridge.java:244)\n\tat org.apache.parquet.hadoop.util.wrapped.io.VectorIoBridge.readVectoredRanges(VectorIoBridge.java:201)\n\tat org.apache.parquet.hadoop.util.H1SeekableInputStream.readVectored(H1SeekableInputStream.java:75)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readVectored(ParquetFileReader.java:1357)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readAllPartsVectoredOrNormal(ParquetFileReader.java:1274)\n\tat org.apache.parquet.hadoop.ParquetFileReader.internalReadRowGroup(ParquetFileReader.java:1185)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:1135)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readNextFilteredRowGroup(ParquetFileReader.java:1380)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase$ParquetRowGroupReaderImpl.readNextRowGroup(SpecificParquetRecordReaderBase.java:292)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:481)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:399)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:238)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:130)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:292)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:130)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:139)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:773)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: java.lang.NoSuchMethodError: 'java.util.List org.apache.hadoop.fs.VectoredReadUtils.validateNonOverlappingAndReturnSortedRanges(java.util.List)'\n\tat org.apache.hadoop.fs.s3a.S3AInputStream.readVectored(S3AInputStream.java:895)\n\tat org.apache.hadoop.fs.FSDataInputStream.readVectored(FSDataInputStream.java:308)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat org.apache.parquet.util.DynMethods$UnboundMethod.invokeChecked(DynMethods.java:62)\n\t... 39 more\n"
     ]
    }
   ],
   "source": [
    "# task 4\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import to_date, col ,date_format , to_timestamp\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"transform3\")\n",
    "\n",
    "    # ✅ Hadoop S3A jars \n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.4.0,\"\n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
    "    )\n",
    "\n",
    "    # MinIO\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9010\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n",
    "#==============================================================\n",
    "# هنجيب الداتا  ونحولها لداتا فريم\n",
    "\n",
    "df = spark.read.parquet(\n",
    "    \"s3a://datalake/processed/Motor_Vehicle_Collisions_Crashes\",\n",
    "    inferSchema=True,\n",
    ")\n",
    "\n",
    "df.show(10)\n",
    "\n",
    "\n",
    "\n",
    "# هنحفظ الفايل الجديد علي س3 في الفولدر بتاع الداتا المتعالجه \n",
    "# df_final.write \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .parquet(\"s3a://datalake/processed/streets_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1304f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2026-01-05 15:07:43.925\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `CRASH_TIME_TS` cannot be resolved. Did you mean one of the following? [`CRASH TIME`, `CRASH DATE`, `LATITUDE`, `LOCATION`, `LONGITUDE`]. SQLSTATE: 42703\", \"context\": {\"file\": \"jdk.internal.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o48.withColumn.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `CRASH_TIME_TS` cannot be resolved. Did you mean one of the following? [`CRASH TIME`, `CRASH DATE`, `LATITUDE`, `LOCATION`, `LONGITUDE`]. SQLSTATE: 42703;\\n'Project [CRASH DATE#17, CRASH TIME#18, BOROUGH#19, ZIP CODE#20, LATITUDE#21, LONGITUDE#22, LOCATION#23, ON STREET NAME#24, CROSS STREET NAME#25, OFF STREET NAME#26, NUMBER OF PERSONS INJURED#27, NUMBER OF PERSONS KILLED#28, NUMBER OF PEDESTRIANS INJURED#29, NUMBER OF PEDESTRIANS KILLED#30, NUMBER OF CYCLIST INJURED#31, NUMBER OF CYCLIST KILLED#32, NUMBER OF MOTORIST INJURED#33, NUMBER OF MOTORIST KILLED#34, CONTRIBUTING FACTOR VEHICLE 1#35, CONTRIBUTING FACTOR VEHICLE 2#36, CONTRIBUTING FACTOR VEHICLE 3#37, CONTRIBUTING FACTOR VEHICLE 4#38, CONTRIBUTING FACTOR VEHICLE 5#39, COLLISION_ID#40, VEHICLE TYPE CODE 1#41, ... 5 more fields]\\n+- Relation [CRASH DATE#17,CRASH TIME#18,BOROUGH#19,ZIP CODE#20,LATITUDE#21,LONGITUDE#22,LOCATION#23,ON STREET NAME#24,CROSS STREET NAME#25,OFF STREET NAME#26,NUMBER OF PERSONS INJURED#27,NUMBER OF PERSONS KILLED#28,NUMBER OF PEDESTRIANS INJURED#29,NUMBER OF PEDESTRIANS KILLED#30,NUMBER OF CYCLIST INJURED#31,NUMBER OF CYCLIST KILLED#32,NUMBER OF MOTORIST INJURED#33,NUMBER OF MOTORIST KILLED#34,CONTRIBUTING FACTOR VEHICLE 1#35,CONTRIBUTING FACTOR VEHICLE 2#36,CONTRIBUTING FACTOR VEHICLE 3#37,CONTRIBUTING FACTOR VEHICLE 4#38,CONTRIBUTING FACTOR VEHICLE 5#39,COLLISION_ID#40,VEHICLE TYPE CODE 1#41,... 4 more fields] csv\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:430)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$failUnresolvedAttribute(CheckAnalysis.scala:166)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\\n\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:936)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:404)\\n\\tat scala.collection.immutable.List.foreach(List.scala:323)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:289)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:241)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:289)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:238)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:238)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:114)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:112)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2306)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:1305)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:231)\\n\\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2191)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumn(Dataset.scala:1844)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumn(Dataset.scala:231)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:430)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$failUnresolvedAttribute(CheckAnalysis.scala:166)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\t\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\\n\\t\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\\n\\t\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:936)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:404)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:323)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:289)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:241)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:289)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:238)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:238)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 24 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/home/ahmed-refat/miniconda3/envs/spark/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"263\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/home/ahmed-refat/miniconda3/envs/spark/lib/python3.10/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-----+\n",
      "|VEHICLE TYPE CODE 1                |count|\n",
      "+-----------------------------------+-----+\n",
      "|Sedan                              |38345|\n",
      "|Station Wagon/Sport Utility Vehicle|27414|\n",
      "|Taxi                               |2483 |\n",
      "|Pick-up Truck                      |2013 |\n",
      "|Bike                               |1780 |\n",
      "|Bus                                |1596 |\n",
      "|Box Truck                          |1249 |\n",
      "|Motorcycle                         |959  |\n",
      "|Moped                              |758  |\n",
      "|Tractor Truck Diesel               |544  |\n",
      "|E-Bike                             |541  |\n",
      "|Van                                |500  |\n",
      "|Ambulance                          |400  |\n",
      "|Standing S                         |342  |\n",
      "|Convertible                        |258  |\n",
      "|Dump                               |255  |\n",
      "|PK                                 |211  |\n",
      "|Garbage or Refuse                  |203  |\n",
      "|E-Scooter                          |173  |\n",
      "|Flat Bed                           |127  |\n",
      "+-----------------------------------+-----+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `CRASH_TIME_TS` cannot be resolved. Did you mean one of the following? [`CRASH TIME`, `CRASH DATE`, `LATITUDE`, `LOCATION`, `LONGITUDE`]. SQLSTATE: 42703;\n'Project [CRASH DATE#17, CRASH TIME#18, BOROUGH#19, ZIP CODE#20, LATITUDE#21, LONGITUDE#22, LOCATION#23, ON STREET NAME#24, CROSS STREET NAME#25, OFF STREET NAME#26, NUMBER OF PERSONS INJURED#27, NUMBER OF PERSONS KILLED#28, NUMBER OF PEDESTRIANS INJURED#29, NUMBER OF PEDESTRIANS KILLED#30, NUMBER OF CYCLIST INJURED#31, NUMBER OF CYCLIST KILLED#32, NUMBER OF MOTORIST INJURED#33, NUMBER OF MOTORIST KILLED#34, CONTRIBUTING FACTOR VEHICLE 1#35, CONTRIBUTING FACTOR VEHICLE 2#36, CONTRIBUTING FACTOR VEHICLE 3#37, CONTRIBUTING FACTOR VEHICLE 4#38, CONTRIBUTING FACTOR VEHICLE 5#39, COLLISION_ID#40, VEHICLE TYPE CODE 1#41, ... 5 more fields]\n+- Relation [CRASH DATE#17,CRASH TIME#18,BOROUGH#19,ZIP CODE#20,LATITUDE#21,LONGITUDE#22,LOCATION#23,ON STREET NAME#24,CROSS STREET NAME#25,OFF STREET NAME#26,NUMBER OF PERSONS INJURED#27,NUMBER OF PERSONS KILLED#28,NUMBER OF PEDESTRIANS INJURED#29,NUMBER OF PEDESTRIANS KILLED#30,NUMBER OF CYCLIST INJURED#31,NUMBER OF CYCLIST KILLED#32,NUMBER OF MOTORIST INJURED#33,NUMBER OF MOTORIST KILLED#34,CONTRIBUTING FACTOR VEHICLE 1#35,CONTRIBUTING FACTOR VEHICLE 2#36,CONTRIBUTING FACTOR VEHICLE 3#37,CONTRIBUTING FACTOR VEHICLE 4#38,CONTRIBUTING FACTOR VEHICLE 5#39,COLLISION_ID#40,VEHICLE TYPE CODE 1#41,... 4 more fields] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m df_fill\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVEHICLE TYPE CODE 1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;241m.\u001b[39mcount() \\\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \\\n\u001b[1;32m      7\u001b[0m   \u001b[38;5;241m.\u001b[39mshow( truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hour\n\u001b[0;32m---> 11\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCRASH_HOUR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhour\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCRASH_TIME_TS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#df_fill.filter(col(\"CRASH TIME\").isNull()).count()\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark/lib/python3.10/site-packages/pyspark/sql/classic/dataframe.py:1647\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1644\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1645\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   1646\u001b[0m     )\n\u001b[0;32m-> 1647\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark/lib/python3.10/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/spark/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:269\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    265\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `CRASH_TIME_TS` cannot be resolved. Did you mean one of the following? [`CRASH TIME`, `CRASH DATE`, `LATITUDE`, `LOCATION`, `LONGITUDE`]. SQLSTATE: 42703;\n'Project [CRASH DATE#17, CRASH TIME#18, BOROUGH#19, ZIP CODE#20, LATITUDE#21, LONGITUDE#22, LOCATION#23, ON STREET NAME#24, CROSS STREET NAME#25, OFF STREET NAME#26, NUMBER OF PERSONS INJURED#27, NUMBER OF PERSONS KILLED#28, NUMBER OF PEDESTRIANS INJURED#29, NUMBER OF PEDESTRIANS KILLED#30, NUMBER OF CYCLIST INJURED#31, NUMBER OF CYCLIST KILLED#32, NUMBER OF MOTORIST INJURED#33, NUMBER OF MOTORIST KILLED#34, CONTRIBUTING FACTOR VEHICLE 1#35, CONTRIBUTING FACTOR VEHICLE 2#36, CONTRIBUTING FACTOR VEHICLE 3#37, CONTRIBUTING FACTOR VEHICLE 4#38, CONTRIBUTING FACTOR VEHICLE 5#39, COLLISION_ID#40, VEHICLE TYPE CODE 1#41, ... 5 more fields]\n+- Relation [CRASH DATE#17,CRASH TIME#18,BOROUGH#19,ZIP CODE#20,LATITUDE#21,LONGITUDE#22,LOCATION#23,ON STREET NAME#24,CROSS STREET NAME#25,OFF STREET NAME#26,NUMBER OF PERSONS INJURED#27,NUMBER OF PERSONS KILLED#28,NUMBER OF PEDESTRIANS INJURED#29,NUMBER OF PEDESTRIANS KILLED#30,NUMBER OF CYCLIST INJURED#31,NUMBER OF CYCLIST KILLED#32,NUMBER OF MOTORIST INJURED#33,NUMBER OF MOTORIST KILLED#34,CONTRIBUTING FACTOR VEHICLE 1#35,CONTRIBUTING FACTOR VEHICLE 2#36,CONTRIBUTING FACTOR VEHICLE 3#37,CONTRIBUTING FACTOR VEHICLE 4#38,CONTRIBUTING FACTOR VEHICLE 5#39,COLLISION_ID#40,VEHICLE TYPE CODE 1#41,... 4 more fields] csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "df_fill.groupBy(\"VEHICLE TYPE CODE 1\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"count\", ascending=False) \\\n",
    "  .show( truncate=False)\n",
    "  \n",
    "from pyspark.sql.functions import hour\n",
    "\n",
    "\n",
    "\n",
    "#df_fill.filter(col(\"CRASH TIME\").isNull()).count()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark)",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
