{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55415040",
   "metadata": {},
   "source": [
    "اول حاجه هنحط الداتا في مونجو دي بي "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9eaa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 41362 documents into MongoDB successfully\n"
     ]
    }
   ],
   "source": [
    "#this is new DAG\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "\n",
    "#  الاتصال بـ MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "#  اختيار Database و Collection\n",
    "db = client[\"nyc_data\"]\n",
    "collection = db[\"nyc_traffic\"]\n",
    "\n",
    "# (اختياري) تفريغ الكوليكشن لو بتجرب\n",
    "# collection.delete_many({})\n",
    "\n",
    "#  قراءة ملف الـ JSON\n",
    "with open(\"nyc_traffic.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# تأكيد إن الداتا Array\n",
    "assert isinstance(data, list), \"JSON file must contain a list of documents\"\n",
    "\n",
    "#  إدخال الدhتا\n",
    "collection.insert_many(data)\n",
    "\n",
    "print(f\" Inserted {len(data)} documents into MongoDB successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf13acc",
   "metadata": {},
   "source": [
    "حولنا الداتا بتتاعت الشوارع من ملف سي اي في اي  ملف  parquet\n",
    "ودي عشان ننوع من مصارد الداتا الي جايلنا "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9960acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "data = pd.read_csv(\"Centerline.csv\")\n",
    "data.to_parquet(\"streets_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99794ffa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b10bd2e",
   "metadata": {},
   "source": [
    "انشاء اول تاسك في الداج وده هنعمل كونكت مع الاس 3 و هنكريت باكت \n",
    "ونعمل فيها 2 فولدر وبعدين نرفع الرو داتا فيها \n",
    "1- هنجيب الداتا  من مونجو دي بي ونرفعها علي هئيه جيسون فايل \n",
    "2- هنجيب الفايل البركيه ونرفعه \n",
    "3- هنجيب الفايل الثالث والاخير ونرفعه الي هو csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e009768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "# اتصال Mongo\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client[\"nyc_data\"]\n",
    "collection = db[\"nyc_traffic\"]\n",
    "\n",
    "#-----------------------------------------------\n",
    "# connect with s3 \n",
    "s3 = boto3.resource(\n",
    "    's3',\n",
    "    endpoint_url='http://localhost:9010',\n",
    "    aws_access_key_id='minioadmin',\n",
    "    aws_secret_access_key='minioadmin123'\n",
    ")\n",
    "Bucket=\"datalake\"\n",
    "\n",
    "#------------------------------------------------\n",
    "# create bucket لو مش موجود \n",
    "try:\n",
    "     s3.create_bucket(Bucket=\"datalake\")\n",
    "except:\n",
    "     pass\n",
    "\n",
    "#------------------------------------------------\n",
    "# create 2 folder (raw , processed)\n",
    "try:\n",
    "     s3.put_object(Bucket=\"datalake\", Key=\"raw/\")\n",
    "     s3.put_object(Bucket=\"datalake\", Key=\"processed/\")\n",
    "except:\n",
    "     pass\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# upload first file to s3 (json file)\n",
    "cursor = collection.find({}, {\"_id\": 0})\n",
    "\n",
    "data = list(cursor)          # in-memory\n",
    "json_bytes = json.dumps(\n",
    "    data,\n",
    "    ensure_ascii=False\n",
    ").encode(\"utf-8\")\n",
    "\n",
    "#load to s3\n",
    "s3.Bucket(\"datalake\").put_object(\n",
    "    Key=\"raw/mongo_traffic.json\",\n",
    "    Body=json_bytes,\n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "\n",
    "\n",
    "#---------------------------------------------------==\n",
    "# upload second file csv to s3\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='http://localhost:9010',\n",
    "    aws_access_key_id='minioadmin',\n",
    "    aws_secret_access_key='minioadmin123'\n",
    ")\n",
    "\n",
    "\n",
    "s3_client.upload_file(\n",
    "    Filename=\"Motor_Vehicle_Collisions_Crashes.csv\",  # المسار المحلي للفايل\n",
    "    Bucket=\"datalake\",                    # اسم الباكيت في S3/MinIO\n",
    "    Key=\"raw/Motor_Vehicle_Collisions_Crashes.csv\",    # المسار داخل الباكيت\n",
    "    ExtraArgs={\"ContentType\": \"text/csv\"}   # دي زي ميتا داتا بعرفه ايه الكونتنت الي جوا الفايل ده \n",
    "\n",
    ")\n",
    "\n",
    "#--------------------------------------\n",
    "# upload the last file (parquet file )\n",
    "\n",
    "s3_client.upload_file(\n",
    "    Filename=\"streets_data.parquet\",  # المسار المحلي للفايل\n",
    "    Bucket=\"datalake\",                    # اسم الباكيت في S3/MinIO\n",
    "    Key=\"raw/streets_data.parquet\" ,   # المسار داخل الباكيت\n",
    "    ExtraArgs={\"ContentType\": \"application/octet-stream\"}  # دي زي ميتا داتا بعرفه ايه الكونتنت الي جوا الفايل ده \n",
    "\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d7dc27",
   "metadata": {},
   "source": [
    "التاسك التاني هنجيب الداتا بتاعت الجيسون فايل ونحولها لداتا فريم \n",
    "دا بستخدام باي سبارك و نعمل عليها بروسيسنج وبعدين نرفعها علي س3 تاني بس المره ده في فولدر اسمه بروسيس داتا \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6499759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/04 19:36:11 WARN Utils: Your hostname, refat, resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "26/01/04 19:36:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/ahmed-refat/miniconda3/envs/spark/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/ahmed-refat/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/ahmed-refat/.ivy2.5.2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-704627fd-5328-4dc3-b9c2-009a0d02a9a5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.4.0 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.23.19 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.1.3.Final in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      ":: resolution report :: resolve 846ms :: artifacts dl 21ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.4.0 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.1.3.Final from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.23.19 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-704627fd-5328-4dc3-b9c2-009a0d02a9a5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/17ms)\n",
      "26/01/04 19:36:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# task 2\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"transform1\")\n",
    "\n",
    "    # ✅ Hadoop S3A jars \n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.4.0,\"\n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
    "    )\n",
    "\n",
    "    # MinIO\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9010\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "#spark.sparkContext.setLogLevel(\"WARN\")\n",
    "# هنجيب الداتا بقا ونحولها لداتا فريم\n",
    "\n",
    "df = spark.read.json(\n",
    "    \"s3a://datalake/raw/mongo_traffic.json\"\n",
    ")\n",
    "\n",
    "#df.show(20)\n",
    "#----------------------------------------------------------\n",
    "# هنرجع الداتا تاني في الفولدر الي هو processed\n",
    "#  وهنخلي الفايل يبقا parquet  عشان يحتفظ ب الاسيكما احسن من csv \n",
    "\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"s3a://datalake/processed/mongo_traffic\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b12ce97",
   "metadata": {},
   "source": [
    "التاسك التاني \n",
    "ودي عباره عن اننا هنجيب الفايل الي هو csv \n",
    "وهنبدء نتعامل معاه علي pyspark \n",
    " بحيث اننا نعمل بروسيسنج لداتا الي فيه بما يتناسب مع احتياجتنا \n",
    " وفي الاخر نرفع الفايل علي هئيه بركيه فايل علي s3\n",
    "جوا الفولدر الي اسمه processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ccaa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+--------+---------+----------+--------------------+--------------------+--------------------+--------------------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+------------+--------------------+--------------------+-------------------+-------------------+-------------------+\n",
      "|CRASH DATE|CRASH TIME|  BOROUGH|ZIP CODE| LATITUDE| LONGITUDE|            LOCATION|      ON STREET NAME|   CROSS STREET NAME|     OFF STREET NAME|NUMBER OF PERSONS INJURED|NUMBER OF PERSONS KILLED|NUMBER OF PEDESTRIANS INJURED|NUMBER OF PEDESTRIANS KILLED|NUMBER OF CYCLIST INJURED|NUMBER OF CYCLIST KILLED|NUMBER OF MOTORIST INJURED|NUMBER OF MOTORIST KILLED|CONTRIBUTING FACTOR VEHICLE 1|CONTRIBUTING FACTOR VEHICLE 2|CONTRIBUTING FACTOR VEHICLE 3|CONTRIBUTING FACTOR VEHICLE 4|CONTRIBUTING FACTOR VEHICLE 5|COLLISION_ID| VEHICLE TYPE CODE 1| VEHICLE TYPE CODE 2|VEHICLE TYPE CODE 3|VEHICLE TYPE CODE 4|VEHICLE TYPE CODE 5|\n",
      "+----------+----------+---------+--------+---------+----------+--------------------+--------------------+--------------------+--------------------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+------------+--------------------+--------------------+-------------------+-------------------+-------------------+\n",
      "|09/11/2021|      2:39|     NULL|    NULL|     NULL|      NULL|                NULL|WHITESTONE EXPRES...|           20 AVENUE|                NULL|                        2|                       0|                            0|                           0|                        0|                       0|                         2|                        0|         Aggressive Drivin...|                  Unspecified|                         NULL|                         NULL|                         NULL|     4455765|               Sedan|               Sedan|               NULL|               NULL|               NULL|\n",
      "|03/26/2022|     11:45|     NULL|    NULL|     NULL|      NULL|                NULL|QUEENSBORO BRIDGE...|                NULL|                NULL|                        1|                       0|                            0|                           0|                        0|                       0|                         1|                        0|            Pavement Slippery|                         NULL|                         NULL|                         NULL|                         NULL|     4513547|               Sedan|                NULL|               NULL|               NULL|               NULL|\n",
      "|11/01/2023|      1:29| BROOKLYN|   11230| 40.62179|-73.970024|    (40.62179, -7...|       OCEAN PARKWAY|            AVENUE K|                NULL|                        1|                       0|                            0|                           0|                        0|                       0|                         1|                        0|                  Unspecified|                  Unspecified|                  Unspecified|                         NULL|                         NULL|     4675373|               Moped|               Sedan|              Sedan|               NULL|               NULL|\n",
      "|06/29/2022|      6:55|     NULL|    NULL|     NULL|      NULL|                NULL|  THROGS NECK BRIDGE|                NULL|                NULL|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|         Following Too Clo...|                  Unspecified|                         NULL|                         NULL|                         NULL|     4541903|               Sedan|       Pick-up Truck|               NULL|               NULL|               NULL|\n",
      "|09/21/2022|     13:21|     NULL|    NULL|     NULL|      NULL|                NULL|     BROOKLYN BRIDGE|                NULL|                NULL|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|          Passing Too Closely|                  Unspecified|                         NULL|                         NULL|                         NULL|     4566131|Station Wagon/Spo...|                NULL|               NULL|               NULL|               NULL|\n",
      "|04/26/2023|     13:30|     NULL|    NULL|     NULL|      NULL|                NULL|      WEST 54 STREET|                NULL|                NULL|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|                  Unspecified|                  Unspecified|                         NULL|                         NULL|                         NULL|     4623759|               Sedan|           Box Truck|               NULL|               NULL|               NULL|\n",
      "|11/01/2023|      7:12|     NULL|    NULL|     NULL|      NULL|                NULL|HUTCHINSON RIVER ...|                NULL|                NULL|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|         Following Too Clo...|         Driver Inattentio...|                         NULL|                         NULL|                         NULL|     4675709|               Sedan|Station Wagon/Spo...|               NULL|               NULL|               NULL|\n",
      "|11/01/2023|      8:01|     NULL|    NULL|     NULL|      NULL|                NULL|      WEST 35 STREET|  HENRY HUDSON RIVER|                NULL|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|         Failure to Yield ...|                         NULL|                         NULL|                         NULL|                         NULL|     4675769|               Sedan|                NULL|               NULL|               NULL|               NULL|\n",
      "|04/26/2023|     22:20|     NULL|    NULL|     NULL|      NULL|                NULL|                NULL|                NULL|61        Ed Koch...|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|                  Unspecified|                         NULL|                         NULL|                         NULL|                         NULL|     4623865|               Sedan|       Pick-up Truck|               NULL|               NULL|               NULL|\n",
      "|09/11/2021|      9:35| BROOKLYN|   11208|40.667202|  -73.8665|    (40.667202, -...|                NULL|                NULL|1211      LORING ...|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|                  Unspecified|                         NULL|                         NULL|                         NULL|                         NULL|     4456314|               Sedan|                NULL|               NULL|               NULL|               NULL|\n",
      "|12/14/2021|      8:13| BROOKLYN|   11233|40.683304|-73.917274|    (40.683304, -...|     SARATOGA AVENUE|      DECATUR STREET|                NULL|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|                         NULL|                         NULL|                         NULL|                         NULL|                         NULL|     4486609|                NULL|                NULL|               NULL|               NULL|               NULL|\n",
      "|04/14/2021|     12:47|     NULL|    NULL|     NULL|      NULL|                NULL|MAJOR DEEGAN EXPR...|                NULL|                NULL|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|                  Unspecified|                  Unspecified|                         NULL|                         NULL|                         NULL|     4407458|                Dump|               Sedan|               NULL|               NULL|               NULL|\n",
      "|12/14/2021|     17:05|     NULL|    NULL|40.709183|-73.956825|    (40.709183, -...|BROOKLYN QUEENS E...|                NULL|                NULL|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|          Passing Too Closely|                  Unspecified|                         NULL|                         NULL|                         NULL|     4486555|               Sedan|Tractor Truck Diesel|               NULL|               NULL|               NULL|\n",
      "|12/14/2021|      8:17|    BRONX|   10475| 40.86816| -73.83148|    (40.86816, -7...|                NULL|                NULL|344       BAYCHES...|                        2|                       0|                            0|                           0|                        0|                       0|                         2|                        0|                  Unspecified|                  Unspecified|                         NULL|                         NULL|                         NULL|     4486660|               Sedan|               Sedan|               NULL|               NULL|               NULL|\n",
      "|12/14/2021|     21:10| BROOKLYN|   11207| 40.67172|  -73.8971|    (40.67172, -7...|                NULL|                NULL|2047      PITKIN ...|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|          Driver Inexperience|                  Unspecified|                         NULL|                         NULL|                         NULL|     4487074|               Sedan|                NULL|               NULL|               NULL|               NULL|\n",
      "|12/14/2021|     14:58|MANHATTAN|   10017| 40.75144| -73.97397|    (40.75144, -7...|            3 AVENUE|      EAST 43 STREET|                NULL|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|          Passing Too Closely|                  Unspecified|                         NULL|                         NULL|                         NULL|     4486519|               Sedan|Station Wagon/Spo...|               NULL|               NULL|               NULL|\n",
      "|12/13/2021|      0:34|     NULL|    NULL|40.701275| -73.88887|    (40.701275, -...|       MYRTLE AVENUE|                NULL|                NULL|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|         Passing or Lane U...|                  Unspecified|                         NULL|                         NULL|                         NULL|     4486934|Station Wagon/Spo...|                NULL|               NULL|               NULL|               NULL|\n",
      "|12/14/2021|     16:50|   QUEENS|   11413|40.675884| -73.75577|    (40.675884, -...|SPRINGFIELD BOULE...|     EAST GATE PLAZA|                NULL|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|           Turning Improperly|                  Unspecified|                         NULL|                         NULL|                         NULL|     4487127|               Sedan|Station Wagon/Spo...|               NULL|               NULL|               NULL|\n",
      "|12/14/2021|      8:30|     NULL|    NULL|     NULL|      NULL|                NULL|            broadway|west 80 street -w...|                NULL|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|         Unsafe Lane Changing|                  Unspecified|                         NULL|                         NULL|                         NULL|     4486634|Station Wagon/Spo...|               Sedan|               NULL|               NULL|               NULL|\n",
      "|12/14/2021|      0:59|     NULL|    NULL| 40.59662| -74.00231|    (40.59662, -7...|        BELT PARKWAY|                NULL|                NULL|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|                 Unsafe Speed|                         NULL|                         NULL|                         NULL|                         NULL|     4486564|               Sedan|                NULL|               NULL|               NULL|               NULL|\n",
      "+----------+----------+---------+--------+---------+----------+--------------------+--------------------+--------------------+--------------------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+------------+--------------------+--------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# task 3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import to_date, col ,date_format , to_timestamp\n",
    "\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"transform2\")\n",
    "\n",
    "    # ✅ Hadoop S3A jars \n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.4.0,\"\n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
    "    )\n",
    "\n",
    "    # MinIO\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9010\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n",
    "#==============================================================\n",
    "# هنجيب الداتا  ونحولها لداتا فريم\n",
    "\n",
    "df = spark.read.csv(\n",
    "    \"s3a://datalake/raw/Motor_Vehicle_Collisions_Crashes.csv\",\n",
    "    inferSchema=True,\n",
    "    header=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# معالجه الداتا الي في الفايل \n",
    "#----------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# تحدبد الاعمده الي محتجنيها بس \n",
    "df_selected=df.select(\"COLLISION_ID\",\"CRASH DATE\",\"CRASH TIME\",\"BOROUGH\",\"VEHICLE TYPE CODE 1\"\n",
    "              ,\"ON STREET NAME\",\"NUMBER OF PERSONS INJURED\",\"NUMBER OF PERSONS KILLED\"\n",
    "              , \"CONTRIBUTING FACTOR VEHICLE 1\") \n",
    "\n",
    "#----------------------------------\n",
    "# تغير الداتا تيب بتاع عمود التاريخ والقيم الي مش هتفع تبقا تاريخ تيبقا نل\n",
    "df1 = df_selected.withColumn(\n",
    "    \"CRASH DATE\",\n",
    "    expr(\"try_to_date(`CRASH DATE`, 'M/d/yyyy')\")\n",
    ")\n",
    "\n",
    "#----------------------------------\n",
    "#  في عمود الديت هنعمل فلتره بحيث نجيب  القيم الي مش بنل \n",
    "df2 = df1.filter(col(\"CRASH DATE\").isNotNull())\n",
    "\n",
    "#----------------------------------\n",
    "# فلتره الداتا لعام 2025 بس \n",
    "df_2025_plus = df2.filter(\n",
    "    col(\"CRASH DATE\") > \"2025-01-01\"\n",
    ")\n",
    "\n",
    "\n",
    "#----------------------------------\n",
    "# هنحاول اننا نعدل القيم الفاضيه بنل دي بعدين ان كان في وقت \n",
    "# بس دلوقتي هنحط  القيم الفاضيه دي unknown\n",
    "df_fill = df_2025_plus.fillna({\n",
    "    \"BOROUGH\": \"UNKNOWN\",          # أي Null في BOROUGH هيتحول لـ \"UNKNOWN\"\n",
    "    \"ON STREET NAME\": \"UNKNOWN STREET\" , # أي Null في ON STREET NAME هيتحول لـ \"UNKNOWN STREET\"\n",
    "    \"NUMBER OF PERSONS INJURED\":\"0\", # عدد الاصابات الي بنل هحط مكانه 0\n",
    "    \"VEHICLE TYPE CODE 1\":\"Sedan\" # عندي 1417 قيمه  بنل هعوضهم  ب اكتر قيمه اتكرت \n",
    "    \n",
    "})\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# تحويل الداتا تايب بتاع العمود الاصابات الي  int \n",
    "df_4 = df_fill.withColumn(\n",
    "    \"NUMBER OF PERSONS INJURED\",\n",
    "    col(\"NUMBER OF PERSONS INJURED\").cast(\"int\")\n",
    ")\n",
    "from pyspark.sql.functions import to_timestamp , date_format\n",
    "#----------------------------------\n",
    "# تحويل الداتا الي في العمود بتاع الوقت لصيغه ديت\n",
    "df_time = df_4.withColumn(\n",
    "    \"CRASH TIME\",\n",
    "    to_timestamp(col(\"CRASH TIME\"), \"H:mm\")\n",
    ")\n",
    "\n",
    "#----------------------------------\n",
    "# HH:MM تحديد ان الديت يبقا بصيغه الساعه والدقائق فقط  \n",
    "df_hour = df_time.withColumn(\n",
    "    \"CRASH TIME\",\n",
    "    date_format(\"CRASH TIME\", \"HH:MM\")\n",
    ")\n",
    "\n",
    "#=========================================\n",
    "# تغير اسماء الاعمده\n",
    "rename_dict = {\n",
    "    \"CRASH DATE\": \"crash_date\",\n",
    "    \"CRASH TIME\": \"crash_time\",\n",
    "    \"ON STREET NAME\": \"street_name\",\n",
    "    \"NUMBER OF PERSONS INJURED\": \"persons_injured\",\n",
    "    \"NUMBER OF PERSONS KILLED\": \"persons_killed\",\n",
    "    \"VEHICLE TYPE CODE 1\": \"vehicle_type\",\n",
    "    \"CONTRIBUTING FACTOR VEHICLE 1\": \"contributing_factor\",\n",
    "    \"COLLISION_ID\": \"collision_id\",\n",
    "    \"BOROUGH\": \"borough\"\n",
    "}\n",
    "\n",
    "for old, new in rename_dict.items():\n",
    "    df_final = df_hour.withColumnRenamed(old, new)\n",
    "\n",
    "df_final.show(10)\n",
    "\n",
    "\n",
    "# هنحفظ الفايل الجديد علي س3 في الفولدر بتاع الداتا المتعالجه \n",
    "# df.write \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .parquet(\"s3a://datalake/processed/mongo_traffic\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1304f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2026-01-05 15:07:43.925\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `CRASH_TIME_TS` cannot be resolved. Did you mean one of the following? [`CRASH TIME`, `CRASH DATE`, `LATITUDE`, `LOCATION`, `LONGITUDE`]. SQLSTATE: 42703\", \"context\": {\"file\": \"jdk.internal.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o48.withColumn.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `CRASH_TIME_TS` cannot be resolved. Did you mean one of the following? [`CRASH TIME`, `CRASH DATE`, `LATITUDE`, `LOCATION`, `LONGITUDE`]. SQLSTATE: 42703;\\n'Project [CRASH DATE#17, CRASH TIME#18, BOROUGH#19, ZIP CODE#20, LATITUDE#21, LONGITUDE#22, LOCATION#23, ON STREET NAME#24, CROSS STREET NAME#25, OFF STREET NAME#26, NUMBER OF PERSONS INJURED#27, NUMBER OF PERSONS KILLED#28, NUMBER OF PEDESTRIANS INJURED#29, NUMBER OF PEDESTRIANS KILLED#30, NUMBER OF CYCLIST INJURED#31, NUMBER OF CYCLIST KILLED#32, NUMBER OF MOTORIST INJURED#33, NUMBER OF MOTORIST KILLED#34, CONTRIBUTING FACTOR VEHICLE 1#35, CONTRIBUTING FACTOR VEHICLE 2#36, CONTRIBUTING FACTOR VEHICLE 3#37, CONTRIBUTING FACTOR VEHICLE 4#38, CONTRIBUTING FACTOR VEHICLE 5#39, COLLISION_ID#40, VEHICLE TYPE CODE 1#41, ... 5 more fields]\\n+- Relation [CRASH DATE#17,CRASH TIME#18,BOROUGH#19,ZIP CODE#20,LATITUDE#21,LONGITUDE#22,LOCATION#23,ON STREET NAME#24,CROSS STREET NAME#25,OFF STREET NAME#26,NUMBER OF PERSONS INJURED#27,NUMBER OF PERSONS KILLED#28,NUMBER OF PEDESTRIANS INJURED#29,NUMBER OF PEDESTRIANS KILLED#30,NUMBER OF CYCLIST INJURED#31,NUMBER OF CYCLIST KILLED#32,NUMBER OF MOTORIST INJURED#33,NUMBER OF MOTORIST KILLED#34,CONTRIBUTING FACTOR VEHICLE 1#35,CONTRIBUTING FACTOR VEHICLE 2#36,CONTRIBUTING FACTOR VEHICLE 3#37,CONTRIBUTING FACTOR VEHICLE 4#38,CONTRIBUTING FACTOR VEHICLE 5#39,COLLISION_ID#40,VEHICLE TYPE CODE 1#41,... 4 more fields] csv\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:430)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$failUnresolvedAttribute(CheckAnalysis.scala:166)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\\n\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:936)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:404)\\n\\tat scala.collection.immutable.List.foreach(List.scala:323)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:289)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:241)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:289)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:238)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:238)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:114)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:112)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2306)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:1305)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:231)\\n\\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2191)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumn(Dataset.scala:1844)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumn(Dataset.scala:231)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:430)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$failUnresolvedAttribute(CheckAnalysis.scala:166)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:406)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\t\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\\n\\t\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\\n\\t\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:936)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:272)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:272)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:404)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:323)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:281)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:273)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:281)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:289)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:241)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:228)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:289)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:238)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:238)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 24 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/home/ahmed-refat/miniconda3/envs/spark/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"263\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/home/ahmed-refat/miniconda3/envs/spark/lib/python3.10/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-----+\n",
      "|VEHICLE TYPE CODE 1                |count|\n",
      "+-----------------------------------+-----+\n",
      "|Sedan                              |38345|\n",
      "|Station Wagon/Sport Utility Vehicle|27414|\n",
      "|Taxi                               |2483 |\n",
      "|Pick-up Truck                      |2013 |\n",
      "|Bike                               |1780 |\n",
      "|Bus                                |1596 |\n",
      "|Box Truck                          |1249 |\n",
      "|Motorcycle                         |959  |\n",
      "|Moped                              |758  |\n",
      "|Tractor Truck Diesel               |544  |\n",
      "|E-Bike                             |541  |\n",
      "|Van                                |500  |\n",
      "|Ambulance                          |400  |\n",
      "|Standing S                         |342  |\n",
      "|Convertible                        |258  |\n",
      "|Dump                               |255  |\n",
      "|PK                                 |211  |\n",
      "|Garbage or Refuse                  |203  |\n",
      "|E-Scooter                          |173  |\n",
      "|Flat Bed                           |127  |\n",
      "+-----------------------------------+-----+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `CRASH_TIME_TS` cannot be resolved. Did you mean one of the following? [`CRASH TIME`, `CRASH DATE`, `LATITUDE`, `LOCATION`, `LONGITUDE`]. SQLSTATE: 42703;\n'Project [CRASH DATE#17, CRASH TIME#18, BOROUGH#19, ZIP CODE#20, LATITUDE#21, LONGITUDE#22, LOCATION#23, ON STREET NAME#24, CROSS STREET NAME#25, OFF STREET NAME#26, NUMBER OF PERSONS INJURED#27, NUMBER OF PERSONS KILLED#28, NUMBER OF PEDESTRIANS INJURED#29, NUMBER OF PEDESTRIANS KILLED#30, NUMBER OF CYCLIST INJURED#31, NUMBER OF CYCLIST KILLED#32, NUMBER OF MOTORIST INJURED#33, NUMBER OF MOTORIST KILLED#34, CONTRIBUTING FACTOR VEHICLE 1#35, CONTRIBUTING FACTOR VEHICLE 2#36, CONTRIBUTING FACTOR VEHICLE 3#37, CONTRIBUTING FACTOR VEHICLE 4#38, CONTRIBUTING FACTOR VEHICLE 5#39, COLLISION_ID#40, VEHICLE TYPE CODE 1#41, ... 5 more fields]\n+- Relation [CRASH DATE#17,CRASH TIME#18,BOROUGH#19,ZIP CODE#20,LATITUDE#21,LONGITUDE#22,LOCATION#23,ON STREET NAME#24,CROSS STREET NAME#25,OFF STREET NAME#26,NUMBER OF PERSONS INJURED#27,NUMBER OF PERSONS KILLED#28,NUMBER OF PEDESTRIANS INJURED#29,NUMBER OF PEDESTRIANS KILLED#30,NUMBER OF CYCLIST INJURED#31,NUMBER OF CYCLIST KILLED#32,NUMBER OF MOTORIST INJURED#33,NUMBER OF MOTORIST KILLED#34,CONTRIBUTING FACTOR VEHICLE 1#35,CONTRIBUTING FACTOR VEHICLE 2#36,CONTRIBUTING FACTOR VEHICLE 3#37,CONTRIBUTING FACTOR VEHICLE 4#38,CONTRIBUTING FACTOR VEHICLE 5#39,COLLISION_ID#40,VEHICLE TYPE CODE 1#41,... 4 more fields] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m df_fill\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVEHICLE TYPE CODE 1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;241m.\u001b[39mcount() \\\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \\\n\u001b[1;32m      7\u001b[0m   \u001b[38;5;241m.\u001b[39mshow( truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hour\n\u001b[0;32m---> 11\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCRASH_HOUR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhour\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCRASH_TIME_TS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#df_fill.filter(col(\"CRASH TIME\").isNull()).count()\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark/lib/python3.10/site-packages/pyspark/sql/classic/dataframe.py:1647\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1644\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1645\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   1646\u001b[0m     )\n\u001b[0;32m-> 1647\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark/lib/python3.10/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/spark/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:269\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    265\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `CRASH_TIME_TS` cannot be resolved. Did you mean one of the following? [`CRASH TIME`, `CRASH DATE`, `LATITUDE`, `LOCATION`, `LONGITUDE`]. SQLSTATE: 42703;\n'Project [CRASH DATE#17, CRASH TIME#18, BOROUGH#19, ZIP CODE#20, LATITUDE#21, LONGITUDE#22, LOCATION#23, ON STREET NAME#24, CROSS STREET NAME#25, OFF STREET NAME#26, NUMBER OF PERSONS INJURED#27, NUMBER OF PERSONS KILLED#28, NUMBER OF PEDESTRIANS INJURED#29, NUMBER OF PEDESTRIANS KILLED#30, NUMBER OF CYCLIST INJURED#31, NUMBER OF CYCLIST KILLED#32, NUMBER OF MOTORIST INJURED#33, NUMBER OF MOTORIST KILLED#34, CONTRIBUTING FACTOR VEHICLE 1#35, CONTRIBUTING FACTOR VEHICLE 2#36, CONTRIBUTING FACTOR VEHICLE 3#37, CONTRIBUTING FACTOR VEHICLE 4#38, CONTRIBUTING FACTOR VEHICLE 5#39, COLLISION_ID#40, VEHICLE TYPE CODE 1#41, ... 5 more fields]\n+- Relation [CRASH DATE#17,CRASH TIME#18,BOROUGH#19,ZIP CODE#20,LATITUDE#21,LONGITUDE#22,LOCATION#23,ON STREET NAME#24,CROSS STREET NAME#25,OFF STREET NAME#26,NUMBER OF PERSONS INJURED#27,NUMBER OF PERSONS KILLED#28,NUMBER OF PEDESTRIANS INJURED#29,NUMBER OF PEDESTRIANS KILLED#30,NUMBER OF CYCLIST INJURED#31,NUMBER OF CYCLIST KILLED#32,NUMBER OF MOTORIST INJURED#33,NUMBER OF MOTORIST KILLED#34,CONTRIBUTING FACTOR VEHICLE 1#35,CONTRIBUTING FACTOR VEHICLE 2#36,CONTRIBUTING FACTOR VEHICLE 3#37,CONTRIBUTING FACTOR VEHICLE 4#38,CONTRIBUTING FACTOR VEHICLE 5#39,COLLISION_ID#40,VEHICLE TYPE CODE 1#41,... 4 more fields] csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "df_fill.groupBy(\"VEHICLE TYPE CODE 1\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"count\", ascending=False) \\\n",
    "  .show( truncate=False)\n",
    "  \n",
    "from pyspark.sql.functions import hour\n",
    "\n",
    "\n",
    "\n",
    "#df_fill.filter(col(\"CRASH TIME\").isNull()).count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77fd8781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------------+-------+-------------------+-----------------+-------------------------+------------------------+-----------------------------+\n",
      "|COLLISION_ID|CRASH DATE|         CRASH TIME|BOROUGH|VEHICLE TYPE CODE 1|   ON STREET NAME|NUMBER OF PERSONS INJURED|NUMBER OF PERSONS KILLED|CONTRIBUTING FACTOR VEHICLE 1|\n",
      "+------------+----------+-------------------+-------+-------------------+-----------------+-------------------------+------------------------+-----------------------------+\n",
      "|     4803256|2025-04-02|1970-01-01 09:30:00| QUEENS|          Ambulance|   UNKNOWN STREET|                        0|                       0|                  Unspecified|\n",
      "|     4804525|2025-04-07|1970-01-01 12:40:00|  BRONX|              Sedan|BEDFORD PARK BLVD|                        0|                       0|                  Unspecified|\n",
      "|     4804466|2025-04-07|1970-01-01 06:00:00|  BRONX|              Sedan|   UNKNOWN STREET|                        0|                       0|                 Unsafe Speed|\n",
      "|     4790397|2025-01-31|1970-01-01 08:19:00| QUEENS|              Sedan|        EXETER ST|                        1|                       0|         Driver Inattentio...|\n",
      "|     4790246|2025-02-02|1970-01-01 20:36:00|UNKNOWN|              Sedan|   UNKNOWN STREET|                        3|                       0|         Aggressive Drivin...|\n",
      "+------------+----------+-------------------+-------+-------------------+-----------------+-------------------------+------------------------+-----------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_time.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9328eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:===============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(df_4.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f11720f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark)",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
