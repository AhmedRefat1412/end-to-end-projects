{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508b7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# قراءه من ملف csv \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"pocessing data \").getOrCreate()\n",
    "\n",
    "df= spark.read.csv(\n",
    "    \"data.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be033c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# فهم الداتا مهم قبل اي عمليه كلين عليها \n",
    "\n",
    "# هنشوف الاعمده\n",
    "df.columns\n",
    "\n",
    "#عدد الصفوف \n",
    "df.count\n",
    "\n",
    "#القيم  الفاضيه \n",
    "\n",
    "from pyspark.sql.functions import col \n",
    "\n",
    "df.select([\n",
    "    col(c).isNull().alias(c) for c in df.columns]\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36038e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#حذف اعمده انا مش محتجها \n",
    "df = df.drop(\"unnecessary_coulmn\",\"temp_col\")\n",
    "#  او ممكن نعمل سليكت لاعمده الي محتجنها بس \n",
    "df= df.select(\"id\",\"name\",\"age\",\"date\")\n",
    "\n",
    "# =============================================\n",
    "# حذف الصفوف الي فيها قيم فاضيه \n",
    "\n",
    "#حذف اي صف فيه null\n",
    "df_clean= df.dropna()\n",
    "\n",
    "#حذف صفوف علي اعمده معينه بس يعني احذف الصف لو  عند العمود الفلاني ملهوش فيمه او قيمه بنل \n",
    "df_clean = df.dropna(subset=[\"id\",\"date\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056be727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ملي القيم الفاضيه  بطرق مختلفه \n",
    "\n",
    "# ملي القيم كلها بقيمه واحده \n",
    "df=df.fillna(0)\n",
    "\n",
    "# ملي حسب العمود\n",
    "df=df.fillna({\n",
    "    \"amount\":0,\n",
    "    \"name\":\"unknown\"\n",
    "})\n",
    "\n",
    "# ملي علي حسب المتوسط(mean)\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "mean_value= df.select(avg(\"amount\")).first()[0]\n",
    "df=df.fillna({\n",
    "    \"amount\":mean_value\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32440279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# تغير نوع الدتا (data type casting)\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df=df.withColumn(\n",
    "    'amount',\n",
    "    col(\"amount\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# تحويل تاريخ\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df=df.withColumn(\n",
    "    \"order_date\",\n",
    "    to_date(col(\"order_date\")),\n",
    "    \"yyy-MM-dd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# فلتره الداتا (filter/scope)\n",
    "\n",
    "# فلتره شرطيه \n",
    "df=df.filter(col(\"amount\")>1000)\n",
    "\n",
    "# فلتره بتاريخ \n",
    "df =df.filter(\n",
    "    col(\"order_date\")>= \"1-1-2024\"\n",
    ")\n",
    "\n",
    "# الفلتره بتاريخين محددين او بين فتره معينه \n",
    "df=df.filter(\n",
    "    (col(\"order_date\")>= \"1-1-2024\")&\n",
    "    (col(\"order_date\")<= \"30-6-2024\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572ada95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# انشاء اعمده جديده \n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df= df.withColumn(\n",
    "    \"amount_category\" ,# اسم العمود \n",
    "    when(col(\"amount\") >= 1000,\"high\") # القيم الي هتبقا في العمود علي حسب الشرط بتاعنا \n",
    "    .when(col(\"amount\")>=500, \"medium\")\n",
    "    .otherwise(\"low\")\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ac13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ازاله القيم المكرره \n",
    "df= df.dropDuplicates()\n",
    "\n",
    "# ازاله القيم المكرره علي حسب عمود معين \n",
    "df=df.drop_duplicates([\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b56592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agregations \n",
    "\n",
    "df.groupBy(\"amount_category\").count().show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark)",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
