{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8c283b9",
   "metadata": {},
   "source": [
    "في الاول هنعمل كريت ل الداتا ونخزنها في مونجو دي بي "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204615d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 20 users\n",
      "Inserted 20 products\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import random\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"my_database\"]\n",
    "\n",
    "# ----- Prepare first and last names -----\n",
    "first_names = [\"Ahmed\", \"Ali\", \"Sara\", \"Mona\", \"Omar\", \"Youssef\", \"Nour\", \"Laila\", \"Khaled\", \"Hana\"]\n",
    "last_names = [\"Refat\", \"Hassan\", \"Ibrahim\", \"Mohamed\", \"Farouk\", \"Adel\", \"Sami\", \"Nabil\", \"Tarek\", \"Fathy\"]\n",
    "\n",
    "# ----- Generate Users -----\n",
    "users = []\n",
    "for i in range(1, 21):  # 20 users\n",
    "    user = {\n",
    "        \"user_id\": i,\n",
    "        \"name\": f\"{random.choice(first_names)} {random.choice(last_names)}\",\n",
    "        \"email\": f\"user{i}@example.com\",\n",
    "        \"age\": random.randint(18, 60)\n",
    "    }\n",
    "    users.append(user)\n",
    "\n",
    "# Insert users\n",
    "db[\"users\"].insert_many(users)\n",
    "print(\"Inserted 20 users\")\n",
    "\n",
    "# ----- Generate Products -----\n",
    "products = []\n",
    "categories = [\"Electronics\", \"Books\", \"Clothes\", \"Sports\"]\n",
    "for i in range(1, 21):  # 20 products\n",
    "    product = {\n",
    "        \"product_id\": i,\n",
    "        \"name\": f\"Product {i}\",\n",
    "        \"category\": random.choice(categories),\n",
    "        \"price\": round(random.uniform(10, 500), 2)\n",
    "    }\n",
    "    products.append(product)\n",
    "\n",
    "# Insert products\n",
    "db[\"products\"].insert_many(products)\n",
    "print(\"Inserted 20 products\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reviews = []\n",
    "for i in range(1, 21):  # 20 reviews\n",
    "    review = {\n",
    "        \"review_id\": i,\n",
    "        \"user_id\": random.randint(1, 20),\n",
    "        \"product_id\": random.randint(1, 20),\n",
    "        \"rating\": random.randint(1, 5),\n",
    "        \"comment\": f\"This is a comment for product {random.randint(1, 20)}\"\n",
    "    }\n",
    "    reviews.append(review)\n",
    "\n",
    "# Insert reviews\n",
    "db[\"reviews\"].insert_many(reviews)\n",
    "print(\"Inserted 20 reviews\")\n",
    "\n",
    "\n",
    "orders = []\n",
    "for i in range(1, 21):  # 20 orders\n",
    "    order = {\n",
    "        \"order_id\": i,\n",
    "        \"user_id\": random.randint(1, 20),  # user_id موجود\n",
    "        \"products\": [\n",
    "            {\"product_id\": random.randint(1, 20), \"quantity\": random.randint(1, 5)}\n",
    "            for _ in range(random.randint(1, 3))  # كل order فيها 1-3 منتجات\n",
    "        ],\n",
    "        \"total\": 0  # هنحسبها بعدين\n",
    "    }\n",
    "    # حساب total\n",
    "    order[\"total\"] = sum([p[\"quantity\"] * round(random.uniform(10, 500), 2) for p in order[\"products\"]])\n",
    "    orders.append(order)\n",
    "\n",
    "# Insert orders\n",
    "db[\"orders\"].insert_many(orders)\n",
    "print(\"Inserted 20 orders\")\n",
    "orders = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e6ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create csv files \n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# table 1\n",
    "users_sql = pd.DataFrame({\n",
    "    \"user_id\": range(1, 21),\n",
    "    \"name\": [f\"User_{i}\" for i in range(1, 21)],\n",
    "    \"email\": [f\"user{i}@sql.com\" for i in range(1, 21)],\n",
    "    \"signup_date\": pd.date_range(\"2025-01-01\", periods=20)\n",
    "})\n",
    "users_sql.to_csv(\"users_sql.csv\", index=False)\n",
    "\n",
    "\n",
    "# table 2\n",
    "products_sql = pd.DataFrame({\n",
    "    \"product_id\": range(1, 21),\n",
    "    \"name\": [f\"Product_{i}\" for i in range(1, 21)],\n",
    "    \"category\": [random.choice([\"Electronics\", \"Books\", \"Clothing\"]) for _ in range(20)],\n",
    "    \"price\": [round(random.uniform(10, 500), 2) for _ in range(20)]\n",
    "})\n",
    "products_sql.to_csv(\"products_sql.csv\", index=False)\n",
    "\n",
    "# table 3\n",
    "\n",
    "orders_sql = []\n",
    "for i in range(1, 21):\n",
    "    orders_sql.append({\n",
    "        \"order_id\": i,\n",
    "        \"user_id\": random.randint(1, 20),\n",
    "        \"product_id\": random.randint(1, 20),\n",
    "        \"quantity\": random.randint(1, 5)\n",
    "    })\n",
    "\n",
    "orders_sql_df = pd.DataFrame(orders_sql)\n",
    "orders_sql_df.to_csv(\"orders_sql.csv\", index=False)\n",
    "\n",
    "\n",
    "# table 4\n",
    "\n",
    "reviews_sql = []\n",
    "for i in range(1, 21):\n",
    "    reviews_sql.append({\n",
    "        \"review_id\": i,\n",
    "        \"user_id\": random.randint(1, 20),\n",
    "        \"product_id\": random.randint(1, 20),\n",
    "        \"rating\": random.randint(1, 5),\n",
    "        \"comment\": f\"This is SQL comment for product {random.randint(1, 20)}\"\n",
    "    })\n",
    "\n",
    "reviews_sql_df = pd.DataFrame(reviews_sql)\n",
    "reviews_sql_df.to_csv(\"reviews_sql.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56171f5b",
   "metadata": {},
   "source": [
    "intrgation of data and the processing \n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbe7c88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/26 22:50:23 WARN Utils: Your hostname, ahmed-refat-VirtualBox, resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/11/26 22:50:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/11/26 22:50:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------------+---+\n",
      "|user_id|         name|            email|age|\n",
      "+-------+-------------+-----------------+---+\n",
      "|      1|  Ali Mohamed|user1@example.com| 38|\n",
      "|      2|    Hana Sami|user2@example.com| 51|\n",
      "|      3|   Ahmed Sami|user3@example.com| 28|\n",
      "|      4|Khaled Hassan|user4@example.com| 18|\n",
      "|      5|  Khaled Adel|user5@example.com| 29|\n",
      "+-------+-------------+-----------------+---+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------------------+------------------+\n",
      "|order_id|user_id|            products|             total|\n",
      "+--------+-------+--------------------+------------------+\n",
      "|       1|     14|[{quantity -> 4, ...|           4749.09|\n",
      "|       2|     15|[{quantity -> 2, ...|            757.82|\n",
      "|       3|     14|[{quantity -> 1, ...|           1215.49|\n",
      "|       4|      7|[{quantity -> 3, ...|            827.63|\n",
      "|       5|     12|[{quantity -> 5, ...|2769.6800000000003|\n",
      "+--------+-------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "+----------+---------+--------+------+\n",
      "|product_id|     name|category| price|\n",
      "+----------+---------+--------+------+\n",
      "|         1|Product 1|   Books|438.83|\n",
      "|         2|Product 2|   Books| 44.64|\n",
      "|         3|Product 3| Clothes| 86.02|\n",
      "|         4|Product 4|   Books|327.74|\n",
      "|         5|Product 5|  Sports|335.79|\n",
      "+----------+---------+--------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# اول خطوه قراءه البيانات من mongodb\n",
    "# create  sparksession \n",
    "\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from pyspark.sql   import SparkSession\n",
    "\n",
    "# create  sparksession \n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"firt_ETL\")\n",
    "    .config(\"spark.jars\", \"/home/ahmed-refat/Desktop/end_to_end_projects/postgresql-42.2.5.jar\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# the connection of mongodb local \n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"my_database\"] # دي اسم الداتا بيز بتاعتي في مونجو دي بي \n",
    "\n",
    "collection1= pd.DataFrame(list(db[\"users\"].find())).drop(columns=[\"_id\"])\n",
    "collection2= pd.DataFrame(list(db[\"orders\"].find())).drop(columns=[\"_id\"])\n",
    "collection3= pd.DataFrame(list(db[\"products\"].find())).drop(columns=[\"_id\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# collection قراءه ال \n",
    "df_users =spark.createDataFrame(collection1)\n",
    "df_orders =spark.createDataFrame(collection2)\n",
    "df_products =spark.createDataFrame(collection3)\n",
    "\n",
    "\n",
    "df_users.show(5)\n",
    "df_orders.show(5)\n",
    "df_products.show(5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f46946d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------+--------+\n",
      "|order_id|user_id|product_id|quantity|\n",
      "+--------+-------+----------+--------+\n",
      "|       1|      9|         8|       4|\n",
      "|       2|      3|         7|       2|\n",
      "|       3|      6|        16|       2|\n",
      "|       4|      6|        14|       1|\n",
      "|       5|      3|        18|       1|\n",
      "+--------+-------+----------+--------+\n",
      "only showing top 5 rows\n",
      "+---------+-------+----------+------+--------------------+\n",
      "|review_id|user_id|product_id|rating|             comment|\n",
      "+---------+-------+----------+------+--------------------+\n",
      "|        1|     11|        13|     2|This is SQL comme...|\n",
      "|        2|      4|        11|     2|This is SQL comme...|\n",
      "|        3|     11|         1|     2|This is SQL comme...|\n",
      "|        4|      2|        13|     1|This is SQL comme...|\n",
      "|        5|      8|         6|     4|This is SQL comme...|\n",
      "+---------+-------+----------+------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# هنا بقا هنعمل عمليه تحويل الملفات csv  ل داتا فريم جوا باي سبارك \n",
    "\n",
    "# قراءة ملفات CSV في PySpark\n",
    "df_users_csv = spark.read.csv(\"users_sql.csv\", header=True, inferSchema=True)\n",
    "df_products_csv = spark.read.csv(\"products_sql.csv\", header=True, inferSchema=True)\n",
    "df_orders_csv = spark.read.csv(\"orders_sql.csv\", header=True, inferSchema=True)\n",
    "df_reviews_csv = spark.read.csv(\"reviews_sql.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# عرض أول 5 صفوف لأي ملف للتأكد\n",
    "\n",
    "df_orders_csv.show(5)\n",
    "df_reviews_csv.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0988a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- products: array (nullable = true)\n",
      " |    |-- element: map (containsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: long (valueContainsNull = true)\n",
      " |-- total: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# عمليه دمج الداتا فريمز المتشابها في داتا فريم واحده عن طريق حذف المكرر وعمل جوين عليهم \n",
    "csv_df_clean = df_users_csv.drop(\"name\", \"email\")\n",
    "\n",
    "df_users_clean = df_users.join(\n",
    "    csv_df_clean,\n",
    "    on=\"user_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# دمج جداول المنتجات \n",
    "# لما جيت اجرب لقيت الاتنين نفس الحاجه ف هنشتغل علي الجدول الي جي من مونجو دي بي وخلاص \n",
    "df_product_clean= df_products\n",
    "\n",
    "\n",
    "# دمج جدولين ال orders \n",
    "# بس هيظهر قدمنا مشكله ان الجدول الي جي من المونجو فيه مشكله \n",
    "# وتحديدا ان في عمود اسمه برودكت جواه list \n",
    "\n",
    "\n",
    "\n",
    "df_orders.printSchema()\n",
    "df_orders_csv.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06f745ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create explode to the column of product in df_orders == mongodb\n",
    "from pyspark.sql.functions import col , explode \n",
    "df_explode= (df_orders.withColumn(\"product\",explode(\"products\")))\n",
    "\n",
    "fact_order=(\n",
    "  df_explode\n",
    "  .withColumn(\"product_id\",col(\"product\")[\"product_id\"])\n",
    "  .withColumn(\"quantity\",col(\"product\")[\"quantity\"])\n",
    "  .drop(\"product\", \"products\")\n",
    "\n",
    ")\n",
    "#fact_order.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa6454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect with postgres and add df_users_clean to dim_usres \n",
    "df_users_clean.write.jdbc(\n",
    "    url=\"jdbc:postgresql://localhost:5432/ecommerce_dw\",\n",
    "    table=\"dim_users\",\n",
    "    mode=\"append\",\n",
    "    properties={\n",
    "        \"user\": \"admin\",\n",
    "        \"password\": \"admin\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# connect with postgres and add df_products to dim_products \n",
    "\n",
    "df_products.write.jdbc(\n",
    "   url=\"jdbc:postgresql://localhost:5432/ecommerce_dw\",\n",
    "    table=\"dim_products\",\n",
    "    mode=\"append\",\n",
    "    properties={\n",
    "        \"user\": \"admin\",\n",
    "        \"password\": \"admin\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd90d4bb",
   "metadata": {},
   "source": [
    "git dim tables from  postgres to gir surrogate key and make join than load fact_orders  and fact_review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bacdf77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+--------+------------------+\n",
      "|user_key|product_key|order_id|quantity|             total|\n",
      "+--------+-----------+--------+--------+------------------+\n",
      "|      17|         20|       4|       3|            827.63|\n",
      "|      17|         15|       4|       1|            827.63|\n",
      "|      17|         13|       4|       4|            827.63|\n",
      "|       7|          2|       6|       4|           1428.96|\n",
      "|       7|          8|       6|       2|           1428.96|\n",
      "|      20|         11|       7|       2|            903.46|\n",
      "|       2|          4|       5|       5|2769.6800000000003|\n",
      "|       2|          8|       5|       3|2769.6800000000003|\n",
      "|      18|         20|       9|       1|            2077.2|\n",
      "|      18|          2|       9|       5|            2077.2|\n",
      "|       1|         17|      10|       4|4072.8500000000004|\n",
      "|       1|          1|      10|       5|4072.8500000000004|\n",
      "|       4|         16|       1|       4|           4749.09|\n",
      "|       4|         17|       1|       4|           4749.09|\n",
      "|       4|         12|       1|       3|           4749.09|\n",
      "|       4|         15|       3|       1|           1215.49|\n",
      "|       4|          2|       3|       5|           1215.49|\n",
      "|       5|          8|       2|       2|            757.82|\n",
      "|       5|         13|       8|       3|           1743.54|\n",
      "|       5|          5|       8|       1|           1743.54|\n",
      "+--------+-----------+--------+--------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_users_dim_db = spark.read.jdbc(\n",
    "    url=\"jdbc:postgresql://localhost:5432/ecommerce_dw\",\n",
    "    table=\"dim_users\",\n",
    "    properties={\n",
    "        \"user\": \"admin\",\n",
    "        \"password\": \"admin\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    ")\n",
    "\n",
    "df_products_dim_db = spark.read.jdbc(\n",
    "    url=\"jdbc:postgresql://localhost:5432/ecommerce_dw\",\n",
    "    table=\"dim_products\",\n",
    "    properties={\n",
    "        \"user\": \"admin\",\n",
    "        \"password\": \"admin\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    ")\n",
    "\n",
    "#df_users_dim_db.show()\n",
    "#df_products_dim_db.show()\n",
    "\n",
    "# هنعمل جوين بقا لل fact مع الدايمنشنز التانيه \n",
    "fact_order_db = fact_order.join(\n",
    "    df_users_dim_db.select(\"user_id\", \"user_key\"),\n",
    "    on=\"user_id\",\n",
    "    how=\"left\"\n",
    ").join(\n",
    "    df_products_dim_db.select(\"product_id\",\"product_key\"),\n",
    "    on=\"product_id\",\n",
    "    how=\"left\"    \n",
    ")\n",
    "\n",
    "# هنختار الاعمده الي تتناسب اكتر مع الجدول الي صممناه في الداتا وير هاوس \n",
    "fact_order_final = fact_order_db.select(\n",
    "    \"user_key\",\n",
    "    \"product_key\",\n",
    "    \"order_id\",\n",
    "    \"quantity\",\n",
    "    \"total\",\n",
    ")\n",
    "fact_order_final.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a274a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# connect with postgres and fact_order \n",
    "\n",
    "fact_order_final.write.jdbc(\n",
    "    url=\"jdbc:postgresql://localhost:5432/ecommerce_dw\",\n",
    "    table=\"fact_orders\",\n",
    "    mode=\"append\",\n",
    "    properties={\n",
    "        \"user\": \"admin\",\n",
    "        \"password\": \"admin\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
